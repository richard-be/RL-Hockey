{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if os.environ.get(\"preamble_run\", None) is not None: \n",
    "    print(\"Not re-runnning preamble\")\n",
    "else: \n",
    "    %run -i ../preamble.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from src.mbpo.util import get_latest_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV = \"HalfCheetah-v4\"\n",
    "ENV = \"Hockey-One-v0\"\n",
    "RESULTS_DIR = f\"outputs/mbpo/default/gym___{ENV}\"\n",
    "\n",
    "runs = [\n",
    "    # (\"2026.02.04/130615\", \"real data only\"), \n",
    "    # (\"2026.02.04/130737\", \"MBPO\"),\n",
    "    # (\"2026.02.04/150451\", \"real data only, fixed alpha=0.3\"),\n",
    "    # (\"latest\", \"latest\"),\n",
    "    # (\"2026.02.05/120550\", \"colored noise, SAC\"),\n",
    "    # (\"2026.02.05/120649\", \"colored noise, MBPO\"),\n",
    "    # (\"2026.02.05/131530\", \"colored noise, SAC, update_frq=1\"),\n",
    "    (\"2026.02.05/134007\", \"colored noise, SAC, update_frq=1\"),\n",
    "    # (\"2026.02.05/151110\", \"colored noise, MBPO, update_frq=1, horizon=1\"),\n",
    "    # (\"2026.02.06/115535\", \"cn, MBPO, update_f=1, horizon=1, faked reward prediction\"),\n",
    "    # (\"2026.02.06/172422\", \"cn, MBPO, update_f=1, horizon=1, seperate reward prediction\"),\n",
    "    # (\"2026.02.10/155958\", \"cn, MBPO, update_f=10, horizon=1, manual reward calculation\"),\n",
    "    # (\"2026.02.10/162533\", \"cn, MBPO, update_f=10, horizon=1, manual reward calculation w/ closeness to puck\"), # no term function yet\n",
    "    # (\"2026.02.10/165245\", \"cn, MBPO, update_f=1, horizon=1, manual reward calculation w/ closeness to puck, term fn\"),\n",
    "    # (\"2026.02.11/161214\", \"raw MBPO from mbrlib, update_f=1, horizon=1\"),\n",
    "    (\"2026.02.11/164147\", \"raw MBPO from mbrlib, cn; update_f=1; horizon=1\"),\n",
    "    (\"2026.02.11/165451\", \"raw MBPO from mbrlib, cn; update_f=1; horizon=1; added reward and term fn\"),\n",
    "    (\"2026.02.11/170456\", \"raw MBPO from mbrlib, update_f=1, horizon=1 - rerun because accidentally cancelled\"),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(run): \n",
    "    model_dir = get_latest_run_dir(RESULTS_DIR) if run == \"latest\" else f\"{RESULTS_DIR}/{run}\"\n",
    "    results = pd.read_csv(f\"{model_dir}/results.csv\")\n",
    "    # if os.path.exists(model_dir+\"/model.pth\") or os.path.exists(model_dir+\"/state_model.pth\"): \n",
    "    try: \n",
    "        model_train = pd.read_csv(f\"{model_dir}/model_train.csv\")\n",
    "    except:  \n",
    "        model_train = pd.DataFrame(columns=[\"step\", \"model_loss\", \"model_val_score\"])\n",
    "    # if os.path.exists(model_dir+\"/env_uncertainty.csv\"): \n",
    "    try: \n",
    "        uncertainties = pd.read_csv(f\"{model_dir}/env_uncertainty.csv\")\n",
    "    except: \n",
    "        uncertainties = pd.DataFrame(columns=[\"step\", \"alea_uncert_mean\", \"epis_uncert_mean\", \"n_rejected_predictions\"])\n",
    "    train = pd.read_csv(f\"{model_dir}/train.csv\")\n",
    "\n",
    "    return results, model_train, uncertainties, train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c960dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {run: get_statistics(run) for run, _ in runs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186bbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figs, axs = plt.subplots(5, 2, figsize=(12, 16))\n",
    "\n",
    "fig_rew, (rew_axis, batch_rew_axis) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig_sac, (sac_axis0, sac_axis1, sac_axis2, sac_axis3) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig_model, (model_axis0, model_axis1, model_axis2, model_axis3, model_axis4) = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "figs = [fig_rew, fig_sac, fig_model]\n",
    "# env_model_axis0 = axs[1, 1]\n",
    "# env_model_axis1 = axs[2, 1]\n",
    "# uncert_axis = axs[3, 1]\n",
    "# uncert_axis2 = axs[4, 1]\n",
    "\n",
    "for run_id, run_name in runs: \n",
    "    results, model_train, uncertainties, train = statistics[run_id]\n",
    "\n",
    "    rew_axis.set_title(\"Reward\")\n",
    "    results[\"smoothed_reward\"] = results['episode_reward'].rolling(window=20, min_periods=1).mean()\n",
    "    rew_axis.plot(results['env_step'], results['smoothed_reward'], label=run_name)\n",
    "\n",
    "    sac_axis0.set_title(\"SAC training - Actor loss\")\n",
    "    sac_axis0.plot(train['step'], train['actor_loss'], label=run_name)\n",
    "    \n",
    "    sac_axis1.set_title(\"SAC training - Alpha loss\")\n",
    "    sac_axis1.plot(train['step'], train['alpha_loss'], label=run_name)\n",
    "\n",
    "    sac_axis2.set_title(\"SAC training - Alpha value\")\n",
    "    sac_axis2.plot(train['step'], train['alpha_value'], label=run_name)\n",
    "\n",
    "    sac_axis3.set_title(\"SAC training - Critic loss\")\n",
    "    sac_axis3.plot(train['step'], train['critic_loss'], label=run_name)\n",
    "\n",
    "    batch_rew_axis.set_title(\"Batch reward\")\n",
    "    batch_rew_axis.plot(train['step'], train['batch_reward'], label=run_name)\n",
    "\n",
    "    model_axis0.set_title(\"Env model loss\")\n",
    "    model_axis0.plot(model_train['step'], model_train['model_loss'], label=run_name)\n",
    "    model_axis0.set_yscale(\"log\")\n",
    "\n",
    "    model_axis1.set_title(\"Env model validation score\")\n",
    "    model_axis1.plot(model_train['step'], model_train['model_val_score'], label=run_name)\n",
    "\n",
    "\n",
    "    model_axis2.set_title(\"Mean aleatoric uncertainty\")\n",
    "    model_axis2.plot(uncertainties['step'], uncertainties['alea_uncert_mean'], label=run_name)\n",
    "\n",
    "    model_axis3.set_title(\"Mean epistemic uncertainty\")\n",
    "    model_axis3.plot(uncertainties['step'], uncertainties['epis_uncert_mean'], label=run_name)\n",
    "    \n",
    "    model_axis4.set_title(\"N rejected predictions\")\n",
    "    model_axis4.plot(uncertainties['step'], uncertainties['n_rejected_predictions'], label=run_name)\n",
    "\n",
    "[f.legend([run_name for _, run_name in runs]) for f in figs]\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c2700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hockey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
