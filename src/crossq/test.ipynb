{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f20b58",
   "metadata": {},
   "source": [
    "# Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d973ff51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `Hockey` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameNotFound\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecordEpisodeStatistics, RecordVideo\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m env = \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHockey-v0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/gymnasium/envs/registration.py:681\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    680\u001b[39m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     env_spec = \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/gymnasium/envs/registration.py:526\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(env_id)\u001b[39m\n\u001b[32m    520\u001b[39m     logger.warn(\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m     )\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m    528\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    529\u001b[39m     )\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/gymnasium/envs/registration.py:392\u001b[39m, in \u001b[36m_check_version_exists\u001b[39m\u001b[34m(ns, name, version)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/gymnasium/envs/registration.py:369\u001b[39m, in \u001b[36m_check_name_exists\u001b[39m\u001b[34m(ns, name)\u001b[39m\n\u001b[32m    366\u001b[39m namespace_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m suggestion_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error.NameNotFound(\n\u001b[32m    370\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[31mNameNotFound\u001b[39m: Environment `Hockey` doesn't exist."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "\n",
    "env = gym.envs.make(\"Hockey-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8da3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hockey.hockey_env as h_env\n",
    "\n",
    "env = h_env.HockeyEnv_BasicOpponent(weak_opponent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce00d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    observation, _, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e6f74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 0,\n",
       " 'reward_closeness_to_puck': -0.5198881360799305,\n",
       " 'reward_touch_puck': 0.0,\n",
       " 'reward_puck_direction': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b151b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilist(range(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee452709",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcodec'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VideoDecoder\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec'"
     ]
    }
   ],
   "source": [
    "from torchcodec.decoders import VideoDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c445a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nargizi/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/Users/nargizi/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torchvision/io/video.py:199: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_video  # install av\n",
    "clip = read_video(\"/Users/nargizi/Desktop/Uni/Masters/Reinforcement Learning/Project/RL-Hockey/src/crossq/vid/rl-video-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb411a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb6360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<moviepy.video.io.VideoFileClip.VideoFileClip at 0x110c7dfd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "frames = np.array(list(clip.iter_frames()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1c26ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QNetwork.__init__() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msac\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SACgent, SACConfig\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m agent = \u001b[43mSACgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSACConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m agent.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Uni/Masters/Reinforcement Learning/Project/RL-Hockey/crossq/sac.py:107\u001b[39m, in \u001b[36mSACgent.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    105\u001b[39m obs_dim = np.prod(\u001b[38;5;28mself\u001b[39m.env.observation_space.shape)\n\u001b[32m    106\u001b[39m action_dim = np.prod(\u001b[38;5;28mself\u001b[39m.env.action_space.shape)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28mself\u001b[39m.q_functions = [ \u001b[43mQNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.config.device) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.q_ensamble)]\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m.q_targets = [QNetwork(observation_dim=obs_dim, action_dim=action_dim, hidden_dim=\u001b[32m256\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.config.device) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.q_ensamble)]\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q_func, q_target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.q_functions, \u001b[38;5;28mself\u001b[39m.q_targets):\n",
      "\u001b[31mTypeError\u001b[39m: QNetwork.__init__() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "from sac import SACgent, SACConfig\n",
    "agent = SACgent(SACConfig(device=\"mps\"))\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df205d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6., device='mps:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcrossq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossQAgent, CrossQConfig\n\u001b[32m      2\u001b[39m agent = CrossQAgent(CrossQConfig(device=\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m, env_id=\u001b[33m\"\u001b[39m\u001b[33mHalfCheetah-v5\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Uni/Masters/Reinforcement Learning/Project/RL-Hockey/crossq/crossq.py:299\u001b[39m, in \u001b[36mCrossQAgent.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    295\u001b[39m     episodic_return = \u001b[32m0\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step >= \u001b[38;5;28mself\u001b[39m.config.learning_starts:  \u001b[38;5;66;03m# wait till we have enough data for the full mini-batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Uni/Masters/Reinforcement Learning/Project/RL-Hockey/crossq/crossq.py:243\u001b[39m, in \u001b[36mCrossQAgent.fit\u001b[39m\u001b[34m(self, global_step)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, q_func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.q_functions):\n\u001b[32m    241\u001b[39m         \u001b[38;5;28mself\u001b[39m.writer.add_scalar(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGrad/Q_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, get_gradient_norm(q_func), global_step)\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_step % \u001b[38;5;28mself\u001b[39m.config.policy_delay == \u001b[32m0\u001b[39m:\n\u001b[32m    247\u001b[39m     actor_loss, entropy, values = compute_actor_loss(\u001b[38;5;28mself\u001b[39m.q_functions, \u001b[38;5;28mself\u001b[39m.policy, alpha, observation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/optim/adam.py:237\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     adam(\n\u001b[32m    248\u001b[39m         params_with_grad,\n\u001b[32m    249\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    269\u001b[39m     )\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/optim/adam.py:151\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m         has_complex |= \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m         params_with_grad.append(p)\n\u001b[32m    153\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.is_sparse:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from crossq import CrossQAgent, CrossQConfig\n",
    "agent = CrossQAgent(CrossQConfig(device=\"mps\", env_id=\"HalfCheetah-v5\"))\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb7f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075f6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v5\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(2000):\n",
    "    # this is where you would insert your policy\n",
    "    action = agent.act(observation=observation, deterministic=True)\n",
    "    action = action.detach().cpu().numpy()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84dadd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossq import QNetwork, GaussianPolicy, GaussianPolicyConfig, NNConfig, NormalizationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e309542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "models = torch.load(\"/Users/nargizi/Desktop/Uni/Masters/Reinforcement Learning/Project/RL-Hockey/models/crossq/100000/model.pkl\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4456058",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pool = [1400, 1200, 1300, 900]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "57f565d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "envs = gym.make_vec(\"HalfCheetah-v5\", num_envs=10, vectorization_mode=\"sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "db47f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([True, False, True, False])\n",
    "b = np.array([True, False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "aad707e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.newaxis, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9685b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(a).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9355ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "b = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bb9e3efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[147]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ai, bi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a[:], b[:]):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ai.shape, \u001b[43mbi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for ai, bi in zip(a[:], b[:]):\n",
    "    print(ai.shape, bi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f83fe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = np.array(score_pool)\n",
    "scores = scores / scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b2488d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29166667, 0.25      , 0.27083333, 0.1875    ])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f4947671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(len(score_pool)), p=(scores / scores.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6fb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_funct = QNetwork(NNConfig(17 + 6, 512, 1, normalization_config=NormalizationConfig(type=\"BRN\", warmup_steps=100_000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546f5ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (body): Sequential(\n",
       "    (batchnorm0): BatchRenorm1d()\n",
       "    (dense0): Linear(in_features=23, out_features=512, bias=True)\n",
       "    (act0): ReLU()\n",
       "    (batchnorm1): BatchRenorm1d()\n",
       "    (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (act1): ReLU()\n",
       "    (batchnorm2): BatchRenorm1d()\n",
       "  )\n",
       "  (output_layers): ModuleList(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_funct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1a943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(512 * 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d4cedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998919e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = [(1, \"a\"), (2, \"b\"), (3, \"c\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1afb0fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 'a'), (2, 'b')], [(1, 'a'), (2, 'b')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = itertools.tee(zip([1, 2], [\"a\", \"b\"]))\n",
    "a, b = list(a), list(b)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c53e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31.9599, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_funct.body.dense1.weight.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c85a6931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.0632, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_funct.body.dense1.weight.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24eab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.rand((512, 512))\n",
    "bias = torch.rand(512)\n",
    "\n",
    "def forward(x, weight, bias):\n",
    "    dense = weight @ x + bias\n",
    "    relu = torch.nn.ReLU()(dense)\n",
    "    return relu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6304593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead ReLU % (baseline)            : 1.2250\n",
      "Dead ReLU % (joint scaling c=0.07682687789201736) : 1.2015\n",
      "Dead ReLU % (joint norm target=13) : 1.2221\n",
      "Dead ReLU % (weight-only scaling) : 0.0719\n",
      "Dead ReLU % (separate norming)    : 22.9242\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "D = 512\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "# Fixed layer params\n",
    "weight = torch.rand((D, D), device=device, dtype=dtype) / D\n",
    "bias = torch.rand((D,), device=device, dtype=dtype) \n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "def forward(x, W, b):\n",
    "    z = W @ x + b\n",
    "    y = relu(z)\n",
    "    return y, z\n",
    "\n",
    "def dead_relu_percent_from_z(z: torch.Tensor) -> float:\n",
    "    # dead = output == 0  <=>  z <= 0 for ReLU (ignoring exact zeros)\n",
    "    return (z <= 0).float().mean().item() * 100.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_dead_percent(W, b, n_samples=10_000, x_dist=\"normal\"):\n",
    "    dead = 0.0\n",
    "    for _ in range(n_samples):\n",
    "        if x_dist == \"normal\":\n",
    "            x = torch.randn(D, device=device, dtype=dtype)\n",
    "        elif x_dist == \"uniform\":\n",
    "            x = torch.rand(D, device=device, dtype=dtype) * 2 - 1\n",
    "        else:\n",
    "            raise ValueError(\"x_dist must be 'normal' or 'uniform'\")\n",
    "        _, z = forward(x, W, b)\n",
    "        dead += dead_relu_percent_from_z(z)\n",
    "    return dead / n_samples\n",
    "\n",
    "# ---------- Variants ----------\n",
    "# 0) baseline\n",
    "W0, b0 = weight.clone(), bias.clone()\n",
    "\n",
    "# 1) \"joint scaling\": multiply both by same positive constant c\n",
    "vec = torch.cat([W0.view(-1), b0.view(-1)])\n",
    "c = 1 / vec.norm()\n",
    "W_joint = c * W0\n",
    "b_joint = c * b0\n",
    "\n",
    "# 2) \"joint norm normalization\": scale both together to hit a target joint norm\n",
    "target = 13\n",
    "theta0 = torch.cat([W0.flatten(), b0.flatten()])\n",
    "scale_joint = target / (theta0.norm() + 1e-12)\n",
    "W_jointnorm = W0 * scale_joint\n",
    "b_jointnorm = b0 * scale_joint\n",
    "\n",
    "# 3) \"weight-only normalization\": scale W but keep b fixed  (THIS CAN CHANGE dead%)\n",
    "scale_w_only = 0.1\n",
    "W_wonly = W0 * scale_w_only\n",
    "b_wonly = b0.clone()\n",
    "\n",
    "# 4) \"separate normalization\": normalize W and b with different scalars (CAN CHANGE dead%)\n",
    "target_w = 10.0\n",
    "target_b = 10.0\n",
    "scale_w = target_w / (W0.norm() + 1e-12)\n",
    "scale_b = target_b / (b0.norm() + 1e-12)\n",
    "W_sep = W0 * scale_w\n",
    "b_sep = b0 * scale_b\n",
    "\n",
    "# ---------- Run experiment ----------\n",
    "n = 5000  # increase for tighter estimate\n",
    "\n",
    "base = estimate_dead_percent(W0, b0, n_samples=n)\n",
    "joint = estimate_dead_percent(W_joint, b_joint, n_samples=n)\n",
    "jointnorm = estimate_dead_percent(W_jointnorm, b_jointnorm, n_samples=n)\n",
    "wonly = estimate_dead_percent(W_wonly, b_wonly, n_samples=n)\n",
    "sep = estimate_dead_percent(W_sep, b_sep, n_samples=n)\n",
    "\n",
    "print(f\"Dead ReLU % (baseline)            : {base:.4f}\")\n",
    "print(f\"Dead ReLU % (joint scaling c={c}) : {joint:.4f}\")\n",
    "print(f\"Dead ReLU % (joint norm target={target}) : {jointnorm:.4f}\")\n",
    "print(f\"Dead ReLU % (weight-only scaling) : {wonly:.4f}\")\n",
    "print(f\"Dead ReLU % (separate norming)    : {sep:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3e3103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dense → ReLU → BN ===\n",
      "baseline dead% (pre-BN): 51.2932\n",
      "baseline                             max|Δ|=0.000000e+00  mean|Δ|=0.000000e+00  dead% (pre-BN)=51.2932\n",
      "joint scaling (c=0.1)                max|Δ|=6.151199e-05  mean|Δ|=6.683530e-06  dead% (pre-BN)=51.2932\n",
      "joint norm target=100.0              max|Δ|=1.096725e-05  mean|Δ|=6.576774e-07  dead% (pre-BN)=51.2939\n",
      "weight-only scaling (c=0.1)          max|Δ|=1.125764e+00  mean|Δ|=1.538527e-01  dead% (pre-BN)=38.0608\n",
      "separate norming (W->10.0, b->10.0)  max|Δ|=1.856599e+00  mean|Δ|=2.712933e-01  dead% (pre-BN)=23.9571\n",
      "\n",
      "=== Dense → BN → ReLU ===\n",
      "baseline dead% (pre-BN): 51.2932\n",
      "baseline                             max|Δ|=0.000000e+00  mean|Δ|=0.000000e+00  dead% (pre-BN)=51.2932\n",
      "joint scaling (c=0.1)                max|Δ|=1.454353e-05  mean|Δ|=1.189682e-06  dead% (pre-BN)=51.2932\n",
      "joint norm target=100.0              max|Δ|=5.483627e-06  mean|Δ|=2.082108e-07  dead% (pre-BN)=51.2939\n",
      "weight-only scaling (c=0.1)          max|Δ|=1.430511e-05  mean|Δ|=1.189897e-06  dead% (pre-BN)=38.0608\n",
      "separate norming (W->10.0, b->10.0)  max|Δ|=1.142025e-04  mean|Δ|=1.042147e-05  dead% (pre-BN)=23.9571\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "D = 512\n",
    "B = 256\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "# Fixed params\n",
    "W0 = torch.rand((D, D), device=device, dtype=dtype)\n",
    "b0 = torch.rand((D,), device=device, dtype=dtype)\n",
    "\n",
    "# Input batch (BN needs batch dimension)\n",
    "X = torch.randn((B, D), device=device, dtype=dtype)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "\n",
    "def dead_relu_percent_from_z(z: torch.Tensor) -> float:\n",
    "    return (z <= 0).float().mean().item() * 100.0\n",
    "\n",
    "def make_bn():\n",
    "    # track_running_stats=False -> always uses batch stats; makes comparisons clean\n",
    "    bn = nn.BatchNorm1d(D, affine=True, track_running_stats=False).to(device=device, dtype=dtype)\n",
    "    bn.train()\n",
    "    return bn\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_dense_relu_bn(X, W, b):\n",
    "    bn = make_bn()  # fresh BN so baseline and variant use same init stats/params\n",
    "    z = X @ W.T + b\n",
    "    u = relu(z)\n",
    "    y = bn(u)\n",
    "    return y, z\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_dense_bn_relu(X, W, b):\n",
    "    bn = make_bn()\n",
    "    z = X @ W.T + b\n",
    "    u = bn(z)\n",
    "    y = relu(u)\n",
    "    return y, z\n",
    "\n",
    "# ----- Build variants -----\n",
    "c = 0.1\n",
    "\n",
    "variants = {}\n",
    "\n",
    "# baseline\n",
    "variants[\"baseline\"] = (W0.clone(), b0.clone())\n",
    "\n",
    "# joint scaling (cW, cb)\n",
    "variants[f\"joint scaling (c={c})\"] = (c * W0, c * b0)\n",
    "\n",
    "# joint norm target (scale both to hit target joint norm)\n",
    "target_joint = 100.0\n",
    "theta0 = torch.cat([W0.flatten(), b0.flatten()])\n",
    "scale_joint = target_joint / (theta0.norm() + 1e-12)\n",
    "variants[f\"joint norm target={target_joint}\"] = (W0 * scale_joint, b0 * scale_joint)\n",
    "\n",
    "# weight-only scaling (cW, b)\n",
    "variants[f\"weight-only scaling (c={c})\"] = (c * W0, b0.clone())\n",
    "\n",
    "# separate norming (different scalars)\n",
    "target_w = 10.0\n",
    "target_b = 10.0\n",
    "scale_w = target_w / (W0.norm() + 1e-12)\n",
    "scale_b = target_b / (b0.norm() + 1e-12)\n",
    "variants[f\"separate norming (W->{target_w}, b->{target_b})\"] = (W0 * scale_w, b0 * scale_b)\n",
    "\n",
    "def compare_block(block_name, forward_fn):\n",
    "    print(f\"\\n=== {block_name} ===\")\n",
    "\n",
    "    # baseline output\n",
    "    y_base, z_base = forward_fn(X, *variants[\"baseline\"])\n",
    "    dead_base = dead_relu_percent_from_z(z_base)\n",
    "    print(f\"baseline dead% (pre-BN): {dead_base:.4f}\")\n",
    "\n",
    "    for name, (W, b) in variants.items():\n",
    "        y, z = forward_fn(X, W, b)\n",
    "        dead = dead_relu_percent_from_z(z)\n",
    "\n",
    "        max_abs = (y - y_base).abs().max().item()\n",
    "        mean_abs = (y - y_base).abs().mean().item()\n",
    "\n",
    "        print(f\"{name:35s}  max|Δ|={max_abs:.6e}  mean|Δ|={mean_abs:.6e}  dead% (pre-BN)={dead:.4f}\")\n",
    "\n",
    "compare_block(\"Dense → ReLU → BN\", forward_dense_relu_bn)\n",
    "compare_block(\"Dense → BN → ReLU\", forward_dense_bn_relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5fe95c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', 'body', 'body.batchnorm0', 'body.dense0', 'body.act0', 'body.batchnorm1', 'body.dense1', 'body.act1', 'body.batchnorm2', 'output_layers', 'output_layers.0'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(q_funct.named_modules()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "432b0bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body.batchnorm0.weight',\n",
       " 'body.batchnorm0.bias',\n",
       " 'body.dense0.weight',\n",
       " 'body.dense0.bias',\n",
       " 'body.batchnorm1.weight',\n",
       " 'body.batchnorm1.bias',\n",
       " 'body.dense1.weight',\n",
       " 'body.dense1.bias',\n",
       " 'body.batchnorm2.weight',\n",
       " 'body.batchnorm2.bias']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, param in q_funct.named_parameters() if not \"output_layers\" in name ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e75ab403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.weight', '0.bias']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, param in q_funct.output_layers.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f3e085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=23, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in q_funct.named_modules():\n",
    "    if \"dense\" in name:\n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13769abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(296.1057)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = torch.cat([weight.view(-1), bias.view(-1)])\n",
    "vec.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cf1aa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.0645, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([q_funct.body.dense1.weight,\n",
    "q_funct.body.dense1.bias.unsqueeze(-1)], dim=1).norm(p=\"fro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9d9f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_funct.load_state_dict(models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a04683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1620, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_funct.body.dense0.bias.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d2dfd70",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Bool",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq_funct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdense0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/_tensor.py:871\u001b[39m, in \u001b[36mTensor.norm\u001b[39m\u001b[34m(self, p, dim, keepdim, dtype)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    868\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    869\u001b[39m         Tensor.norm, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, p=p, dim=dim, keepdim=keepdim, dtype=dtype\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/functional.py:1790\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(input, p, dim, keepdim, out, dtype)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p == \u001b[33m\"\u001b[39m\u001b[33mfro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1787\u001b[39m     dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch.SymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) <= \u001b[32m2\u001b[39m\n\u001b[32m   1788\u001b[39m ):\n\u001b[32m   1789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1794\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.linalg.vector_norm(\n\u001b[32m   1795\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[32m2\u001b[39m, _dim, keepdim, dtype=dtype, out=out\n\u001b[32m   1796\u001b[39m         )\n",
      "\u001b[31mRuntimeError\u001b[39m: linalg.vector_norm: Expected a floating point or complex tensor as input. Got Bool"
     ]
    }
   ],
   "source": [
    "(q_funct.body.dense0.bias > 0).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "888f28de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4950)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q_funct.body.dense0.weight > 0).sum() / (512 * 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2a5cdecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['body.batchnorm0.weight', 'body.batchnorm0.bias', 'body.dense0.weight', 'body.dense0.bias', 'body.batchnorm1.weight', 'body.batchnorm1.bias', 'body.dense1.weight', 'body.dense1.bias', 'body.batchnorm2.weight', 'body.batchnorm2.bias', 'output_layers.0.weight', 'output_layers.0.bias'])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(q_funct.named_parameters()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5f1b3292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body.dense0.weight\n",
      "body.dense1.weight\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in q_funct.named_parameters():\n",
    "            # only normalize non-finale dense layer's weights\n",
    "            if \"body\" in name and \"dense\" in name and \"weight\" in name:\n",
    "                    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b3f1ada2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.weight']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, param in q_funct.output_layers.named_parameters() if \"weight\" in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b89b2df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=23, out_features=2048, bias=True)\n",
      "Linear(in_features=2048, out_features=2048, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for mod_name, mod in q_funct.named_modules():\n",
    "    # replicate your old filter but at module-level\n",
    "    if (\"body\" in mod_name) and (\"dense\" in mod_name) and isinstance(mod, torch.nn.Linear):\n",
    "        print(mod)\n",
    "        # project_weight_to_norm_ball_(mod.weight, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da16df3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9129)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "m = torch.rand((10, 10))\n",
    "m.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "823d9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "from torch.optim import SGD\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6f2ce020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=1, bias=True)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(20, 1)\n",
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6bcfaac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nargizi/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "wn_fc = weight_norm(fc,dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "930fd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    wn_fc.weight_g.mul_( 1/ (wn_fc.weight_g.norm()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3719341e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5947, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.weight.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b4ac10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(wn_fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5622bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(200, 20)\n",
    "y = x.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "961dd0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0886, 0.2064, 0.8136, 0.7507, 0.9395, 0.6405, 0.5585, 0.1940, 0.7220,\n",
       "        0.0332, 0.8530, 0.7946, 0.5565, 0.4441, 0.0891, 0.0659, 0.6469, 0.1750,\n",
       "        0.6398, 0.8856])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5cd9eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = wn_fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b87e6a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ef009c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "93fa4b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35106.6602, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "69139f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.weight.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "efd41ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2afc037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2761, -0.1871, -0.2110, -0.2283, -0.2080, -0.1921, -0.2301, -0.2608,\n",
       "         -0.1945, -0.1962, -0.2172, -0.1994, -0.2712, -0.2305, -0.2167, -0.2042,\n",
       "         -0.2150, -0.2342, -0.1838, -0.2785]],\n",
       "       grad_fn=<WeightNormInterfaceBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6c979fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    wn_fc.parametrizations.weight.original0.fill_(torch.sign(wn_fc.parametrizations.weight.original0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f2da9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n = wn_fc.parametrizations.weight.original0.norm()\n",
    "    wn_fc.parametrizations.weight.original0.mul_(1 / (n + 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c2d75b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.parametrizations.weight.original0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b8a444ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.parametrizations.weight.original1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7a199fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_fc.weight.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801762c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m / m.norm()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5d9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.875e-09"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3e-4 / (400 ** 2)  \n",
    "#  w_new / ||w_new|| = w / ||w|| - norm * ete_eff * grad\n",
    "#  ||w_new - w || / ||w|| = ||w|| *eta_eff  ** ||grad||\n",
    "#  ||w - w_new || / (||w||^2 * ||grad||) = eta_eff\n",
    "#  w_new = w - eta * grad\n",
    "# eta || grad || / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e793da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body.dense0.weight', 'body.dense1.weight', 'output_layers.0.weight']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer_name for layer_name in dict(q_funct.named_parameters()).keys() if (\"dense\" in layer_name or \"output_layers\" in layer_name) and \"weight\" in layer_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b315ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', 'body', 'body.batchnorm0', 'body.dense0', 'body.act0', 'body.batchnorm1', 'body.dense1', 'body.act1', 'body.batchnorm2', 'output_layers', 'output_layers.0'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(q_funct.named_modules()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72fcabfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['body.batchnorm0.weight', 'body.batchnorm0.bias', 'body.dense0.weight', 'body.dense0.bias', 'body.batchnorm1.weight', 'body.batchnorm1.bias', 'body.dense1.weight', 'body.dense1.bias', 'body.batchnorm2.weight', 'body.batchnorm2.bias', 'output_layers.0.weight', 'output_layers.0.bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(q_funct.named_parameters()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7781af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GaussianPolicy(torch.tensor([0, 0, 0, 0, 0, 0]), torch.tensor([1, 1, 1, 1, 1, 1]), GaussianPolicyConfig(17, 256, action_dim=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14c00f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.load_state_dict(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3efcdfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': {'weight': Parameter containing:\n",
       "  tensor([0.9385, 1.7857, 0.8400, 0.4868, 0.3540, 0.8495, 0.6044, 0.3469, 0.9546,\n",
       "          0.6458, 0.9515, 0.9454, 0.5609, 0.2762, 0.7820, 0.5235, 0.2850],\n",
       "         requires_grad=True),\n",
       "  'bias': Parameter containing:\n",
       "  tensor([ 0.1370,  0.1721,  0.0551, -0.1129,  0.1324, -0.2459, -0.1588,  0.0132,\n",
       "           1.2855,  0.0092,  0.0883, -0.0728,  0.0528, -0.1133, -0.0657, -0.0842,\n",
       "          -0.1793], requires_grad=True)},\n",
       " '_buffers': {'running_mean': tensor([-0.0319,  0.1900,  0.0881, -0.2974, -0.1506, -0.1657, -0.0491,  0.0125,\n",
       "           7.8394, -0.0376, -0.0471,  0.0799, -0.0180,  0.0290,  0.0527, -0.0090,\n",
       "           0.0357]),\n",
       "  'running_var': tensor([7.5692e-03, 9.6954e-01, 2.4591e-01, 8.9798e-02, 7.7591e-02, 2.1541e-01,\n",
       "          1.5975e-01, 1.0016e-01, 1.3004e+01, 4.6535e-01, 3.7729e+00, 8.8515e+01,\n",
       "          4.9334e+01, 3.8337e+01, 8.0428e+01, 7.2349e+01, 4.0705e+01]),\n",
       "  'num_batches_tracked': tensor(66667)},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {},\n",
       " 'num_features': 17,\n",
       " 'eps': 1e-05,\n",
       " 'momentum': 0.01,\n",
       " 'affine': True,\n",
       " 'track_running_stats': True}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.body[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6d77778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from memory import Memory\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "756ef396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "90beee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "obs_dim = np.prod(env.observation_space.shape)\n",
    "action_dim = np.prod(env.action_space.shape)\n",
    "q_functions = [ QNetwork(observation_dim=obs_dim, action_dim=action_dim,hidden_dim=2048, \n",
    "                         normalization=torch.nn.BatchNorm1d(num_features=2048, momentum=.99)\n",
    "                         ) for _ in range(2)]\n",
    "policy = GaussianPolicy(min_action=torch.from_numpy(env.action_space.low), max_action=torch.from_numpy((env.action_space.high)), \n",
    "                                     observation_dim=obs_dim, action_dim=action_dim, hidden_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b6b9d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d8a01495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.to(device)\n",
    "q_functions[0].to(device)\n",
    "q_functions[1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2dae0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def compute_critic_loss(q_functions: list[QNetwork], policy: GaussianPolicy, alpha, gamma,\n",
    "                        observations, \n",
    "                        actions, \n",
    "                        rewards, next_observations, finished):\n",
    "    \n",
    "\n",
    "    # return ((q_functions[0].q_value(observations, actions) - rewards) ** 2).mean()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    next_actions, log_prob = policy.get_action(next_observations)\n",
    "    # print(f\"BATCH POLICY {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    qs = torch.stack([q_func.q_value(torch.concat([observations, next_observations]), torch.concat([actions, next_actions])) for q_func in q_functions]).squeeze(-1)\n",
    "    # print(f\"BATCH Q {time.time() - start_time}\")\n",
    "\n",
    "    # print(len(torch.split(qs, 32, dim=1)))\n",
    "    q, next_q = torch.split(qs, qs.shape[1] // 2, dim=1)\n",
    "\n",
    "    next_q = torch.min(next_q, dim=0).values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = next_q - alpha * log_prob.squeeze(-1)\n",
    "\n",
    "    return ((q - (rewards + gamma * next_q * (1 - finished))) ** 2).mean()\n",
    "\n",
    "\n",
    "def compute_actor_loss(q_functions: list[QNetwork], policy: GaussianPolicy, alpha,\n",
    "                        observations, \n",
    "                        actions, \n",
    "                       ):\n",
    "    _, log_prob = policy.get_action(observations)\n",
    "\n",
    "    qs = torch.stack([q_func.q_value(observations, actions) for q_func in q_functions])\n",
    "\n",
    "    q_value = torch.min(qs, dim=0).values\n",
    "\n",
    "    return -(q_value - alpha * log_prob).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dc79f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import  optim\n",
    "q_optimizer = optim.Adam(list(q_functions[0].parameters()) + list(q_functions[1].parameters()), lr=1e-3)\n",
    "actor_optimizer = optim.Adam(list(policy.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b9811edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e235637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation, reward, terminated, action, next_observation = memory.sample(256).T\n",
    "# actions = torch.from_numpy(np.stack(action)).to(device)\n",
    "# next_observations = torch.from_numpy(np.stack(next_observation)).to(device)\n",
    "# observations = torch.from_numpy(np.stack(observation)).to(device)\n",
    "# terminateds = torch.from_numpy(np.stack(terminated)).to(torch.float32).to(device)\n",
    "# rewards = torch.from_numpy(np.stack(reward)).to(torch.float32).to(device)\n",
    "\n",
    "# q_functions[0].q_value(observations, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c17ceaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0859bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITIC LOSS: 8.505910873413086\n",
      "CRITIC LOSS: 902.74462890625\n",
      "CRITIC LOSS: 202.15206909179688\n",
      "CRITIC LOSS: 73.65800476074219\n",
      "CRITIC LOSS: 27.107715606689453\n",
      "CRITIC LOSS: 18.44860076904297\n",
      "CRITIC LOSS: 30.350191116333008\n",
      "CRITIC LOSS: 42.38352584838867\n",
      "CRITIC LOSS: 74.83926391601562\n",
      "CRITIC LOSS: 43.504878997802734\n",
      "CRITIC LOSS: 71.53836822509766\n",
      "CRITIC LOSS: 47.712860107421875\n",
      "CRITIC LOSS: 48.17757034301758\n",
      "CRITIC LOSS: 48.51508712768555\n",
      "CRITIC LOSS: 48.545021057128906\n",
      "CRITIC LOSS: 34.487884521484375\n",
      "CRITIC LOSS: 29.749479293823242\n",
      "CRITIC LOSS: 30.60016632080078\n",
      "CRITIC LOSS: 24.960281372070312\n",
      "CRITIC LOSS: 25.058393478393555\n",
      "CRITIC LOSS: 21.530534744262695\n",
      "CRITIC LOSS: 19.566797256469727\n",
      "CRITIC LOSS: 17.91006851196289\n",
      "CRITIC LOSS: 17.08014678955078\n",
      "CRITIC LOSS: 15.794222831726074\n",
      "CRITIC LOSS: 15.91260051727295\n",
      "CRITIC LOSS: 16.36075782775879\n",
      "CRITIC LOSS: 20.131052017211914\n",
      "CRITIC LOSS: 23.71172523498535\n",
      "CRITIC LOSS: 23.54078483581543\n",
      "CRITIC LOSS: 19.01782989501953\n",
      "CRITIC LOSS: 17.23443603515625\n",
      "CRITIC LOSS: 15.85185718536377\n",
      "CRITIC LOSS: 14.158716201782227\n",
      "CRITIC LOSS: 13.544301986694336\n",
      "CRITIC LOSS: 14.139777183532715\n",
      "CRITIC LOSS: 12.607471466064453\n",
      "CRITIC LOSS: 11.714364051818848\n",
      "CRITIC LOSS: 10.641053199768066\n",
      "CRITIC LOSS: 9.413901329040527\n",
      "CRITIC LOSS: 8.676069259643555\n",
      "CRITIC LOSS: 9.040945053100586\n",
      "CRITIC LOSS: 8.471074104309082\n",
      "CRITIC LOSS: 8.183433532714844\n",
      "CRITIC LOSS: 7.680852890014648\n",
      "CRITIC LOSS: 7.657680511474609\n",
      "CRITIC LOSS: 7.480866432189941\n",
      "CRITIC LOSS: 7.248830318450928\n",
      "CRITIC LOSS: 7.408685684204102\n",
      "CRITIC LOSS: 7.302212715148926\n",
      "CRITIC LOSS: 6.882692813873291\n",
      "CRITIC LOSS: 6.891027927398682\n",
      "CRITIC LOSS: 6.931273937225342\n",
      "CRITIC LOSS: 6.489438533782959\n",
      "CRITIC LOSS: 7.058259010314941\n",
      "CRITIC LOSS: 7.66024112701416\n",
      "CRITIC LOSS: 8.345527648925781\n",
      "CRITIC LOSS: 8.433537483215332\n",
      "CRITIC LOSS: 7.770884037017822\n",
      "CRITIC LOSS: 8.310365676879883\n",
      "CRITIC LOSS: 8.123917579650879\n",
      "CRITIC LOSS: 9.126845359802246\n",
      "CRITIC LOSS: 8.584684371948242\n",
      "CRITIC LOSS: 8.998248100280762\n",
      "CRITIC LOSS: 9.076739311218262\n",
      "CRITIC LOSS: 9.593457221984863\n",
      "CRITIC LOSS: 9.440942764282227\n",
      "CRITIC LOSS: 10.209836959838867\n",
      "CRITIC LOSS: 10.60735034942627\n",
      "CRITIC LOSS: 11.864415168762207\n",
      "CRITIC LOSS: 12.960769653320312\n",
      "CRITIC LOSS: 11.767134666442871\n",
      "CRITIC LOSS: 12.830899238586426\n",
      "CRITIC LOSS: 13.725005149841309\n",
      "CRITIC LOSS: 15.152298927307129\n",
      "CRITIC LOSS: 16.573593139648438\n",
      "CRITIC LOSS: 16.833335876464844\n",
      "CRITIC LOSS: 17.913179397583008\n",
      "CRITIC LOSS: 18.19074249267578\n",
      "CRITIC LOSS: 18.610118865966797\n",
      "CRITIC LOSS: 19.57548713684082\n",
      "CRITIC LOSS: 19.363441467285156\n",
      "CRITIC LOSS: 19.71581268310547\n",
      "CRITIC LOSS: 22.116361618041992\n",
      "CRITIC LOSS: 21.21365737915039\n",
      "CRITIC LOSS: 21.624074935913086\n",
      "CRITIC LOSS: 22.888986587524414\n",
      "CRITIC LOSS: 22.926897048950195\n",
      "CRITIC LOSS: 24.237548828125\n",
      "CRITIC LOSS: 27.315204620361328\n",
      "CRITIC LOSS: 28.777822494506836\n",
      "CRITIC LOSS: 29.296972274780273\n",
      "CRITIC LOSS: 29.466270446777344\n",
      "CRITIC LOSS: 30.554380416870117\n",
      "CRITIC LOSS: 30.582609176635742\n",
      "CRITIC LOSS: 35.954551696777344\n",
      "CRITIC LOSS: 34.62879180908203\n",
      "CRITIC LOSS: 35.432132720947266\n",
      "CRITIC LOSS: 36.192344665527344\n",
      "CRITIC LOSS: 34.07107925415039\n",
      "CRITIC LOSS: 33.25352096557617\n",
      "CRITIC LOSS: 36.6265754699707\n",
      "CRITIC LOSS: 36.66083526611328\n",
      "CRITIC LOSS: 41.284690856933594\n",
      "CRITIC LOSS: 40.100914001464844\n",
      "CRITIC LOSS: 40.52132034301758\n",
      "CRITIC LOSS: 47.77186584472656\n",
      "CRITIC LOSS: 41.72217559814453\n",
      "CRITIC LOSS: 49.32386779785156\n",
      "CRITIC LOSS: 48.43037414550781\n",
      "CRITIC LOSS: 50.60603713989258\n",
      "CRITIC LOSS: 54.814971923828125\n",
      "CRITIC LOSS: 55.64892578125\n",
      "CRITIC LOSS: 60.39718246459961\n",
      "CRITIC LOSS: 60.14542770385742\n",
      "CRITIC LOSS: 60.89616775512695\n",
      "CRITIC LOSS: 65.97110748291016\n",
      "CRITIC LOSS: 55.309295654296875\n",
      "CRITIC LOSS: 60.933162689208984\n",
      "CRITIC LOSS: 69.99590301513672\n",
      "CRITIC LOSS: 68.52536010742188\n",
      "CRITIC LOSS: 64.32937622070312\n",
      "CRITIC LOSS: 63.52093505859375\n",
      "CRITIC LOSS: 70.32105255126953\n",
      "CRITIC LOSS: 81.2729263305664\n",
      "CRITIC LOSS: 74.5460205078125\n",
      "CRITIC LOSS: 71.69641876220703\n",
      "CRITIC LOSS: 84.36103820800781\n",
      "CRITIC LOSS: 80.78047180175781\n",
      "CRITIC LOSS: 76.16299438476562\n",
      "CRITIC LOSS: 86.23477172851562\n",
      "CRITIC LOSS: 85.34790802001953\n",
      "CRITIC LOSS: 88.10845184326172\n",
      "CRITIC LOSS: 89.10735321044922\n",
      "CRITIC LOSS: 84.74566650390625\n",
      "CRITIC LOSS: 82.3145523071289\n",
      "CRITIC LOSS: 90.33495330810547\n",
      "CRITIC LOSS: 86.57278442382812\n",
      "CRITIC LOSS: 86.13274383544922\n",
      "CRITIC LOSS: 91.11780548095703\n",
      "CRITIC LOSS: 91.57254028320312\n",
      "CRITIC LOSS: 103.19376373291016\n",
      "CRITIC LOSS: 104.74778747558594\n",
      "CRITIC LOSS: 105.24691009521484\n",
      "CRITIC LOSS: 108.79469299316406\n",
      "CRITIC LOSS: 119.676513671875\n",
      "CRITIC LOSS: 124.11302185058594\n",
      "CRITIC LOSS: 144.1245880126953\n",
      "CRITIC LOSS: 135.91993713378906\n",
      "CRITIC LOSS: 137.46200561523438\n",
      "CRITIC LOSS: 139.05203247070312\n",
      "CRITIC LOSS: 129.5991668701172\n",
      "CRITIC LOSS: 130.63461303710938\n",
      "CRITIC LOSS: 119.75853729248047\n",
      "CRITIC LOSS: 120.28941345214844\n",
      "CRITIC LOSS: 126.7332992553711\n",
      "CRITIC LOSS: 144.56411743164062\n",
      "CRITIC LOSS: 120.476318359375\n",
      "CRITIC LOSS: 122.73806762695312\n",
      "CRITIC LOSS: 122.26339721679688\n",
      "CRITIC LOSS: 155.08709716796875\n",
      "CRITIC LOSS: 126.08562469482422\n",
      "CRITIC LOSS: 152.36158752441406\n",
      "CRITIC LOSS: 128.29812622070312\n",
      "CRITIC LOSS: 149.32301330566406\n",
      "CRITIC LOSS: 138.50027465820312\n",
      "CRITIC LOSS: 138.67828369140625\n",
      "CRITIC LOSS: 152.1322479248047\n",
      "CRITIC LOSS: 139.4974822998047\n",
      "CRITIC LOSS: 190.97935485839844\n",
      "CRITIC LOSS: 154.50563049316406\n",
      "CRITIC LOSS: 195.0603485107422\n",
      "CRITIC LOSS: 165.0580291748047\n",
      "CRITIC LOSS: 169.6610107421875\n",
      "CRITIC LOSS: 184.40805053710938\n",
      "CRITIC LOSS: 170.8418426513672\n",
      "CRITIC LOSS: 207.23748779296875\n",
      "CRITIC LOSS: 183.05130004882812\n",
      "CRITIC LOSS: 179.92726135253906\n",
      "CRITIC LOSS: 217.17820739746094\n",
      "CRITIC LOSS: 197.55873107910156\n",
      "CRITIC LOSS: 215.88229370117188\n",
      "CRITIC LOSS: 208.080810546875\n",
      "CRITIC LOSS: 209.86090087890625\n",
      "CRITIC LOSS: 230.75399780273438\n",
      "CRITIC LOSS: 231.10775756835938\n",
      "CRITIC LOSS: 212.5723419189453\n",
      "CRITIC LOSS: 247.04891967773438\n",
      "CRITIC LOSS: 275.1001892089844\n",
      "CRITIC LOSS: 241.7630615234375\n",
      "CRITIC LOSS: 235.75265502929688\n",
      "CRITIC LOSS: 252.95559692382812\n",
      "CRITIC LOSS: 255.68069458007812\n",
      "CRITIC LOSS: 264.452392578125\n",
      "CRITIC LOSS: 250.59925842285156\n",
      "CRITIC LOSS: 254.6112823486328\n",
      "CRITIC LOSS: 251.2536163330078\n",
      "CRITIC LOSS: 262.9642333984375\n",
      "CRITIC LOSS: 253.51304626464844\n",
      "CRITIC LOSS: 256.116455078125\n",
      "CRITIC LOSS: 279.1896667480469\n",
      "CRITIC LOSS: 305.5059814453125\n",
      "CRITIC LOSS: 300.978759765625\n",
      "CRITIC LOSS: 321.7225036621094\n",
      "CRITIC LOSS: 292.8067932128906\n",
      "CRITIC LOSS: 274.5526123046875\n",
      "CRITIC LOSS: 260.62322998046875\n",
      "CRITIC LOSS: 399.2626037597656\n",
      "CRITIC LOSS: 368.6106262207031\n",
      "CRITIC LOSS: 334.5050964355469\n",
      "CRITIC LOSS: 335.16668701171875\n",
      "CRITIC LOSS: 275.8482971191406\n",
      "CRITIC LOSS: 259.3404235839844\n",
      "CRITIC LOSS: 294.25701904296875\n",
      "CRITIC LOSS: 339.28399658203125\n",
      "CRITIC LOSS: 269.51129150390625\n",
      "CRITIC LOSS: 347.2196044921875\n",
      "CRITIC LOSS: 285.5320739746094\n",
      "CRITIC LOSS: 334.381103515625\n",
      "CRITIC LOSS: 278.239501953125\n",
      "CRITIC LOSS: 314.9195861816406\n",
      "CRITIC LOSS: 285.7508544921875\n",
      "CRITIC LOSS: 339.7657470703125\n",
      "CRITIC LOSS: 368.1295166015625\n",
      "CRITIC LOSS: 308.43011474609375\n",
      "CRITIC LOSS: 365.1273498535156\n",
      "CRITIC LOSS: 308.7340393066406\n",
      "CRITIC LOSS: 395.3634033203125\n",
      "CRITIC LOSS: 294.41619873046875\n",
      "CRITIC LOSS: 378.81951904296875\n",
      "CRITIC LOSS: 351.9244384765625\n",
      "CRITIC LOSS: 338.87579345703125\n",
      "CRITIC LOSS: 416.6962585449219\n",
      "CRITIC LOSS: 344.9619445800781\n",
      "CRITIC LOSS: 447.49652099609375\n",
      "CRITIC LOSS: 398.91357421875\n",
      "CRITIC LOSS: 399.3020324707031\n",
      "CRITIC LOSS: 430.9355773925781\n",
      "CRITIC LOSS: 408.3074951171875\n",
      "CRITIC LOSS: 434.9477233886719\n",
      "CRITIC LOSS: 431.8115234375\n",
      "CRITIC LOSS: 418.8819274902344\n",
      "CRITIC LOSS: 450.2457275390625\n",
      "CRITIC LOSS: 432.9150085449219\n",
      "CRITIC LOSS: 414.9905090332031\n",
      "CRITIC LOSS: 521.8811645507812\n",
      "CRITIC LOSS: 482.64312744140625\n",
      "CRITIC LOSS: 440.8769226074219\n",
      "CRITIC LOSS: 452.1235656738281\n",
      "CRITIC LOSS: 498.71923828125\n",
      "CRITIC LOSS: 477.7073974609375\n",
      "CRITIC LOSS: 483.63836669921875\n",
      "CRITIC LOSS: 479.8429260253906\n",
      "CRITIC LOSS: 503.9799499511719\n",
      "CRITIC LOSS: 482.75616455078125\n",
      "CRITIC LOSS: 446.4378356933594\n",
      "CRITIC LOSS: 476.55670166015625\n",
      "CRITIC LOSS: 436.6177062988281\n",
      "CRITIC LOSS: 430.8966064453125\n",
      "CRITIC LOSS: 390.20526123046875\n",
      "CRITIC LOSS: 421.83331298828125\n",
      "CRITIC LOSS: 416.96875\n",
      "CRITIC LOSS: 414.5130615234375\n",
      "CRITIC LOSS: 373.3792724609375\n",
      "CRITIC LOSS: 398.2704162597656\n",
      "CRITIC LOSS: 459.9259948730469\n",
      "CRITIC LOSS: 703.4360961914062\n",
      "CRITIC LOSS: 643.0054321289062\n",
      "CRITIC LOSS: 415.95916748046875\n",
      "CRITIC LOSS: 401.7775573730469\n",
      "CRITIC LOSS: 502.95733642578125\n",
      "CRITIC LOSS: 486.2894287109375\n",
      "CRITIC LOSS: 463.1040344238281\n",
      "CRITIC LOSS: 440.4315185546875\n",
      "CRITIC LOSS: 535.3685913085938\n",
      "CRITIC LOSS: 443.0535888671875\n",
      "CRITIC LOSS: 480.36810302734375\n",
      "CRITIC LOSS: 449.7705993652344\n",
      "CRITIC LOSS: 441.1200866699219\n",
      "CRITIC LOSS: 408.88958740234375\n",
      "CRITIC LOSS: 454.4352111816406\n",
      "CRITIC LOSS: 387.81585693359375\n",
      "CRITIC LOSS: 456.39703369140625\n",
      "CRITIC LOSS: 419.74658203125\n",
      "CRITIC LOSS: 424.67376708984375\n",
      "CRITIC LOSS: 470.30755615234375\n",
      "CRITIC LOSS: 428.1109619140625\n",
      "CRITIC LOSS: 459.08099365234375\n",
      "CRITIC LOSS: 422.8104553222656\n",
      "CRITIC LOSS: 473.0470886230469\n",
      "CRITIC LOSS: 419.96868896484375\n",
      "CRITIC LOSS: 480.8663330078125\n",
      "CRITIC LOSS: 436.9727478027344\n",
      "CRITIC LOSS: 480.830078125\n",
      "CRITIC LOSS: 466.0377197265625\n",
      "CRITIC LOSS: 443.2510986328125\n",
      "CRITIC LOSS: 465.6480712890625\n",
      "CRITIC LOSS: 427.541259765625\n",
      "CRITIC LOSS: 427.81707763671875\n",
      "CRITIC LOSS: 444.26495361328125\n",
      "CRITIC LOSS: 446.1751708984375\n",
      "CRITIC LOSS: 469.3809814453125\n",
      "CRITIC LOSS: 429.39117431640625\n",
      "CRITIC LOSS: 454.73760986328125\n",
      "CRITIC LOSS: 445.4908447265625\n",
      "CRITIC LOSS: 462.0555419921875\n",
      "CRITIC LOSS: 476.0281982421875\n",
      "CRITIC LOSS: 409.36773681640625\n",
      "CRITIC LOSS: 468.9061279296875\n",
      "CRITIC LOSS: 448.40411376953125\n",
      "CRITIC LOSS: 472.3843994140625\n",
      "CRITIC LOSS: 464.1531982421875\n",
      "CRITIC LOSS: 440.885986328125\n",
      "CRITIC LOSS: 468.6278076171875\n",
      "CRITIC LOSS: 428.8080139160156\n",
      "CRITIC LOSS: 467.54852294921875\n",
      "CRITIC LOSS: 463.4643859863281\n",
      "CRITIC LOSS: 449.773193359375\n",
      "CRITIC LOSS: 451.4284362792969\n",
      "CRITIC LOSS: 455.616455078125\n",
      "CRITIC LOSS: 444.13287353515625\n",
      "CRITIC LOSS: 477.38360595703125\n",
      "CRITIC LOSS: 435.40594482421875\n",
      "CRITIC LOSS: 419.31292724609375\n",
      "CRITIC LOSS: 466.33282470703125\n",
      "CRITIC LOSS: 454.69854736328125\n",
      "CRITIC LOSS: 437.5379638671875\n",
      "CRITIC LOSS: 430.34991455078125\n",
      "CRITIC LOSS: 442.4871520996094\n",
      "CRITIC LOSS: 460.6270751953125\n",
      "CRITIC LOSS: 435.7356262207031\n",
      "CRITIC LOSS: 421.6728515625\n",
      "CRITIC LOSS: 435.360107421875\n",
      "CRITIC LOSS: 433.3957214355469\n",
      "CRITIC LOSS: 449.1193542480469\n",
      "CRITIC LOSS: 430.1200256347656\n",
      "CRITIC LOSS: 435.6834716796875\n",
      "CRITIC LOSS: 435.48052978515625\n",
      "CRITIC LOSS: 422.07513427734375\n",
      "CRITIC LOSS: 448.29833984375\n",
      "CRITIC LOSS: 446.1582336425781\n",
      "CRITIC LOSS: 432.1401062011719\n",
      "CRITIC LOSS: 435.97161865234375\n",
      "CRITIC LOSS: 419.945068359375\n",
      "CRITIC LOSS: 458.8465881347656\n",
      "CRITIC LOSS: 422.65509033203125\n",
      "CRITIC LOSS: 433.8697509765625\n",
      "CRITIC LOSS: 419.27386474609375\n",
      "CRITIC LOSS: 409.73876953125\n",
      "CRITIC LOSS: 403.91455078125\n",
      "CRITIC LOSS: 423.98504638671875\n",
      "CRITIC LOSS: 416.84124755859375\n",
      "CRITIC LOSS: 404.2913818359375\n",
      "CRITIC LOSS: 418.49505615234375\n",
      "CRITIC LOSS: 421.7352600097656\n",
      "CRITIC LOSS: 395.200927734375\n",
      "CRITIC LOSS: 406.20220947265625\n",
      "CRITIC LOSS: 408.5440673828125\n",
      "CRITIC LOSS: 399.9429931640625\n",
      "CRITIC LOSS: 400.75927734375\n",
      "CRITIC LOSS: 362.62646484375\n",
      "CRITIC LOSS: 381.64031982421875\n",
      "CRITIC LOSS: 371.1708984375\n",
      "CRITIC LOSS: 387.855224609375\n",
      "CRITIC LOSS: 364.50042724609375\n",
      "CRITIC LOSS: 381.25579833984375\n",
      "CRITIC LOSS: 369.249267578125\n",
      "CRITIC LOSS: 384.6698913574219\n",
      "CRITIC LOSS: 341.4324645996094\n",
      "CRITIC LOSS: 348.9705810546875\n",
      "CRITIC LOSS: 343.231689453125\n",
      "CRITIC LOSS: 441.14117431640625\n",
      "CRITIC LOSS: 518.8306274414062\n",
      "CRITIC LOSS: 488.9901123046875\n",
      "CRITIC LOSS: 371.45367431640625\n",
      "CRITIC LOSS: 374.75152587890625\n",
      "CRITIC LOSS: 525.0267333984375\n",
      "CRITIC LOSS: 1784.4683837890625\n",
      "CRITIC LOSS: 3565.870849609375\n",
      "CRITIC LOSS: 882.1283569335938\n",
      "CRITIC LOSS: 730.21630859375\n",
      "CRITIC LOSS: 2695.505126953125\n",
      "CRITIC LOSS: 543.532958984375\n",
      "CRITIC LOSS: 846.6117553710938\n",
      "CRITIC LOSS: 1175.348388671875\n",
      "CRITIC LOSS: 358.66680908203125\n",
      "CRITIC LOSS: 1637.578125\n",
      "CRITIC LOSS: 381.2713623046875\n",
      "CRITIC LOSS: 968.17919921875\n",
      "CRITIC LOSS: 343.4881896972656\n",
      "CRITIC LOSS: 857.1739501953125\n",
      "CRITIC LOSS: 358.1805114746094\n",
      "CRITIC LOSS: 347.17889404296875\n",
      "CRITIC LOSS: 497.5321044921875\n",
      "CRITIC LOSS: 371.603515625\n",
      "CRITIC LOSS: 460.0543212890625\n",
      "CRITIC LOSS: 427.90509033203125\n",
      "CRITIC LOSS: 333.7020263671875\n",
      "CRITIC LOSS: 412.0394287109375\n",
      "CRITIC LOSS: 342.60595703125\n",
      "CRITIC LOSS: 380.0113525390625\n",
      "CRITIC LOSS: 323.85882568359375\n",
      "CRITIC LOSS: 394.4344482421875\n",
      "CRITIC LOSS: 318.2388916015625\n",
      "CRITIC LOSS: 355.66583251953125\n",
      "CRITIC LOSS: 313.78814697265625\n",
      "CRITIC LOSS: 342.7510986328125\n",
      "CRITIC LOSS: 326.537353515625\n",
      "CRITIC LOSS: 343.0738525390625\n",
      "CRITIC LOSS: 401.42791748046875\n",
      "CRITIC LOSS: 323.1931457519531\n",
      "CRITIC LOSS: 388.9954833984375\n",
      "CRITIC LOSS: 361.3409423828125\n",
      "CRITIC LOSS: 352.2938232421875\n",
      "CRITIC LOSS: 405.40948486328125\n",
      "CRITIC LOSS: 361.2063903808594\n",
      "CRITIC LOSS: 391.84832763671875\n",
      "CRITIC LOSS: 328.272705078125\n",
      "CRITIC LOSS: 392.8817443847656\n",
      "CRITIC LOSS: 353.9010009765625\n",
      "CRITIC LOSS: 343.4005126953125\n",
      "CRITIC LOSS: 355.62359619140625\n",
      "CRITIC LOSS: 333.55914306640625\n",
      "CRITIC LOSS: 350.9859924316406\n",
      "CRITIC LOSS: 377.9122314453125\n",
      "CRITIC LOSS: 341.163330078125\n",
      "CRITIC LOSS: 352.63031005859375\n",
      "CRITIC LOSS: 401.8590087890625\n",
      "CRITIC LOSS: 367.2926940917969\n",
      "CRITIC LOSS: 373.7488708496094\n",
      "CRITIC LOSS: 373.4721374511719\n",
      "CRITIC LOSS: 363.8137512207031\n",
      "CRITIC LOSS: 358.73388671875\n",
      "CRITIC LOSS: 375.17864990234375\n",
      "CRITIC LOSS: 349.13232421875\n",
      "CRITIC LOSS: 360.054443359375\n",
      "CRITIC LOSS: 358.54461669921875\n",
      "CRITIC LOSS: 353.5705261230469\n",
      "CRITIC LOSS: 363.4857482910156\n",
      "CRITIC LOSS: 352.6754150390625\n",
      "CRITIC LOSS: 347.2321472167969\n",
      "CRITIC LOSS: 360.3134460449219\n",
      "CRITIC LOSS: 339.0619812011719\n",
      "CRITIC LOSS: 367.5579833984375\n",
      "CRITIC LOSS: 331.25482177734375\n",
      "CRITIC LOSS: 351.5628967285156\n",
      "CRITIC LOSS: 343.88262939453125\n",
      "CRITIC LOSS: 336.46380615234375\n",
      "CRITIC LOSS: 337.40875244140625\n",
      "CRITIC LOSS: 344.11517333984375\n",
      "CRITIC LOSS: 352.3656921386719\n",
      "CRITIC LOSS: 358.5137939453125\n",
      "CRITIC LOSS: 334.0716247558594\n",
      "CRITIC LOSS: 335.013427734375\n",
      "CRITIC LOSS: 350.1650390625\n",
      "CRITIC LOSS: 319.6573486328125\n",
      "CRITIC LOSS: 311.8178405761719\n",
      "CRITIC LOSS: 306.7353820800781\n",
      "CRITIC LOSS: 326.4803161621094\n",
      "CRITIC LOSS: 338.8115234375\n",
      "CRITIC LOSS: 332.27166748046875\n",
      "CRITIC LOSS: 322.0079040527344\n",
      "CRITIC LOSS: 320.365234375\n",
      "CRITIC LOSS: 319.53564453125\n",
      "CRITIC LOSS: 293.48724365234375\n",
      "CRITIC LOSS: 313.1370544433594\n",
      "CRITIC LOSS: 317.13201904296875\n",
      "CRITIC LOSS: 306.873046875\n",
      "CRITIC LOSS: 303.95855712890625\n",
      "CRITIC LOSS: 302.71588134765625\n",
      "CRITIC LOSS: 310.3603820800781\n",
      "CRITIC LOSS: 324.07818603515625\n",
      "CRITIC LOSS: 315.89801025390625\n",
      "CRITIC LOSS: 315.6051940917969\n",
      "CRITIC LOSS: 297.5916442871094\n",
      "CRITIC LOSS: 309.7530517578125\n",
      "CRITIC LOSS: 324.66326904296875\n",
      "CRITIC LOSS: 313.3646240234375\n",
      "CRITIC LOSS: 305.82025146484375\n",
      "CRITIC LOSS: 313.93780517578125\n",
      "CRITIC LOSS: 306.6196594238281\n",
      "CRITIC LOSS: 306.955810546875\n",
      "CRITIC LOSS: 311.06011962890625\n",
      "CRITIC LOSS: 288.8509216308594\n",
      "CRITIC LOSS: 288.4134216308594\n",
      "CRITIC LOSS: 289.72845458984375\n",
      "CRITIC LOSS: 289.84906005859375\n",
      "CRITIC LOSS: 292.82257080078125\n",
      "CRITIC LOSS: 277.8225402832031\n",
      "CRITIC LOSS: 286.0849609375\n",
      "CRITIC LOSS: 291.718505859375\n",
      "CRITIC LOSS: 268.49993896484375\n",
      "CRITIC LOSS: 259.88824462890625\n",
      "CRITIC LOSS: 280.88623046875\n",
      "CRITIC LOSS: 277.6086730957031\n",
      "CRITIC LOSS: 274.1190185546875\n",
      "CRITIC LOSS: 266.9100036621094\n",
      "CRITIC LOSS: 266.02264404296875\n",
      "CRITIC LOSS: 282.9260559082031\n",
      "CRITIC LOSS: 261.7229919433594\n",
      "CRITIC LOSS: 264.5543212890625\n",
      "CRITIC LOSS: 264.879638671875\n",
      "CRITIC LOSS: 265.14361572265625\n",
      "CRITIC LOSS: 248.656494140625\n",
      "CRITIC LOSS: 250.25653076171875\n",
      "CRITIC LOSS: 250.4223175048828\n",
      "CRITIC LOSS: 270.68109130859375\n",
      "CRITIC LOSS: 265.29132080078125\n",
      "CRITIC LOSS: 252.4708251953125\n",
      "CRITIC LOSS: 245.34774780273438\n",
      "CRITIC LOSS: 253.9492950439453\n",
      "CRITIC LOSS: 258.159423828125\n",
      "CRITIC LOSS: 246.48687744140625\n",
      "CRITIC LOSS: 250.95278930664062\n",
      "CRITIC LOSS: 249.55374145507812\n",
      "CRITIC LOSS: 248.82513427734375\n",
      "CRITIC LOSS: 236.47384643554688\n",
      "CRITIC LOSS: 225.485595703125\n",
      "CRITIC LOSS: 234.34341430664062\n",
      "CRITIC LOSS: 245.32037353515625\n",
      "CRITIC LOSS: 241.32749938964844\n",
      "CRITIC LOSS: 228.43490600585938\n",
      "CRITIC LOSS: 237.35763549804688\n",
      "CRITIC LOSS: 239.5114288330078\n",
      "CRITIC LOSS: 218.72113037109375\n",
      "CRITIC LOSS: 228.8234100341797\n",
      "CRITIC LOSS: 232.42413330078125\n",
      "CRITIC LOSS: 226.90847778320312\n",
      "CRITIC LOSS: 229.7032470703125\n",
      "CRITIC LOSS: 204.73568725585938\n",
      "CRITIC LOSS: 215.783447265625\n",
      "CRITIC LOSS: 227.4855499267578\n",
      "CRITIC LOSS: 207.811279296875\n",
      "CRITIC LOSS: 229.6914520263672\n",
      "CRITIC LOSS: 237.43325805664062\n",
      "CRITIC LOSS: 221.91441345214844\n",
      "CRITIC LOSS: 209.6123046875\n",
      "CRITIC LOSS: 214.490234375\n",
      "CRITIC LOSS: 211.02944946289062\n",
      "CRITIC LOSS: 211.61279296875\n",
      "CRITIC LOSS: 207.64544677734375\n",
      "CRITIC LOSS: 214.20277404785156\n",
      "CRITIC LOSS: 202.371337890625\n",
      "CRITIC LOSS: 209.74649047851562\n",
      "CRITIC LOSS: 207.36146545410156\n",
      "CRITIC LOSS: 213.84548950195312\n",
      "CRITIC LOSS: 224.44992065429688\n",
      "CRITIC LOSS: 214.063232421875\n",
      "CRITIC LOSS: 190.93289184570312\n",
      "CRITIC LOSS: 196.0186004638672\n",
      "CRITIC LOSS: 192.10537719726562\n",
      "CRITIC LOSS: 190.68321228027344\n",
      "CRITIC LOSS: 186.79342651367188\n",
      "CRITIC LOSS: 195.11654663085938\n",
      "CRITIC LOSS: 194.96820068359375\n",
      "CRITIC LOSS: 182.98544311523438\n",
      "CRITIC LOSS: 184.77818298339844\n",
      "CRITIC LOSS: 196.90469360351562\n",
      "CRITIC LOSS: 191.00457763671875\n",
      "CRITIC LOSS: 189.86065673828125\n",
      "CRITIC LOSS: 198.6572265625\n",
      "CRITIC LOSS: 189.9981689453125\n",
      "CRITIC LOSS: 186.85281372070312\n",
      "CRITIC LOSS: 190.86741638183594\n",
      "CRITIC LOSS: 181.87461853027344\n",
      "CRITIC LOSS: 190.18447875976562\n",
      "CRITIC LOSS: 183.86383056640625\n",
      "CRITIC LOSS: 177.39956665039062\n",
      "CRITIC LOSS: 170.31117248535156\n",
      "CRITIC LOSS: 169.46400451660156\n",
      "CRITIC LOSS: 179.34457397460938\n",
      "CRITIC LOSS: 173.63897705078125\n",
      "CRITIC LOSS: 183.75433349609375\n",
      "CRITIC LOSS: 178.99078369140625\n",
      "CRITIC LOSS: 173.87472534179688\n",
      "CRITIC LOSS: 164.4451141357422\n",
      "CRITIC LOSS: 174.2112579345703\n",
      "CRITIC LOSS: 170.86431884765625\n",
      "CRITIC LOSS: 171.50686645507812\n",
      "CRITIC LOSS: 164.43365478515625\n",
      "CRITIC LOSS: 191.05490112304688\n",
      "CRITIC LOSS: 163.56011962890625\n",
      "CRITIC LOSS: 157.87576293945312\n",
      "CRITIC LOSS: 173.6024932861328\n",
      "CRITIC LOSS: 166.4783172607422\n",
      "CRITIC LOSS: 160.5985107421875\n",
      "CRITIC LOSS: 164.4548797607422\n",
      "CRITIC LOSS: 151.3568115234375\n",
      "CRITIC LOSS: 149.8040771484375\n",
      "CRITIC LOSS: 162.1804656982422\n",
      "CRITIC LOSS: 154.48631286621094\n",
      "CRITIC LOSS: 150.48239135742188\n",
      "CRITIC LOSS: 142.01266479492188\n",
      "CRITIC LOSS: 147.35009765625\n",
      "CRITIC LOSS: 158.06381225585938\n",
      "CRITIC LOSS: 144.79502868652344\n",
      "CRITIC LOSS: 153.11962890625\n",
      "CRITIC LOSS: 156.35455322265625\n",
      "CRITIC LOSS: 148.93487548828125\n",
      "CRITIC LOSS: 153.79443359375\n",
      "CRITIC LOSS: 130.97601318359375\n",
      "CRITIC LOSS: 136.54818725585938\n",
      "CRITIC LOSS: 140.07437133789062\n",
      "CRITIC LOSS: 140.35971069335938\n",
      "CRITIC LOSS: 151.514892578125\n",
      "CRITIC LOSS: 130.21127319335938\n",
      "CRITIC LOSS: 141.31622314453125\n",
      "CRITIC LOSS: 126.6306381225586\n",
      "CRITIC LOSS: 141.09629821777344\n",
      "CRITIC LOSS: 125.03851318359375\n",
      "CRITIC LOSS: 133.92709350585938\n",
      "CRITIC LOSS: 130.74478149414062\n",
      "CRITIC LOSS: 131.2164306640625\n",
      "CRITIC LOSS: 124.33441162109375\n",
      "CRITIC LOSS: 129.41311645507812\n",
      "CRITIC LOSS: 135.33853149414062\n",
      "CRITIC LOSS: 127.86417388916016\n",
      "CRITIC LOSS: 141.69158935546875\n",
      "CRITIC LOSS: 143.29150390625\n",
      "CRITIC LOSS: 134.39886474609375\n",
      "CRITIC LOSS: 129.56463623046875\n",
      "CRITIC LOSS: 138.91287231445312\n",
      "CRITIC LOSS: 119.49996948242188\n",
      "CRITIC LOSS: 119.91395568847656\n",
      "CRITIC LOSS: 125.3358383178711\n",
      "CRITIC LOSS: 131.23724365234375\n",
      "CRITIC LOSS: 124.30291748046875\n",
      "CRITIC LOSS: 130.8926239013672\n",
      "CRITIC LOSS: 124.40046691894531\n",
      "CRITIC LOSS: 118.78972625732422\n",
      "CRITIC LOSS: 131.5979461669922\n",
      "CRITIC LOSS: 117.64704895019531\n",
      "CRITIC LOSS: 121.8498306274414\n",
      "CRITIC LOSS: 106.78902435302734\n",
      "CRITIC LOSS: 117.67567443847656\n",
      "CRITIC LOSS: 122.63937377929688\n",
      "CRITIC LOSS: 125.20698547363281\n",
      "CRITIC LOSS: 110.59101104736328\n",
      "CRITIC LOSS: 110.67298126220703\n",
      "CRITIC LOSS: 110.01582336425781\n",
      "CRITIC LOSS: 119.44591522216797\n",
      "CRITIC LOSS: 129.80592346191406\n",
      "CRITIC LOSS: 110.02593994140625\n",
      "CRITIC LOSS: 110.36489868164062\n",
      "CRITIC LOSS: 113.95759582519531\n",
      "CRITIC LOSS: 113.74906921386719\n",
      "CRITIC LOSS: 108.37340545654297\n",
      "CRITIC LOSS: 100.5576171875\n",
      "CRITIC LOSS: 102.18606567382812\n",
      "CRITIC LOSS: 122.1832275390625\n",
      "CRITIC LOSS: 109.728271484375\n",
      "CRITIC LOSS: 107.22157287597656\n",
      "CRITIC LOSS: 97.72752380371094\n",
      "CRITIC LOSS: 109.39326477050781\n",
      "CRITIC LOSS: 99.11050415039062\n",
      "CRITIC LOSS: 100.77022552490234\n",
      "CRITIC LOSS: 104.02778625488281\n",
      "CRITIC LOSS: 107.18423461914062\n",
      "CRITIC LOSS: 110.43419647216797\n",
      "CRITIC LOSS: 113.21357727050781\n",
      "CRITIC LOSS: 104.86325073242188\n",
      "CRITIC LOSS: 104.7459716796875\n",
      "CRITIC LOSS: 96.82939147949219\n",
      "CRITIC LOSS: 101.49746704101562\n",
      "CRITIC LOSS: 103.97142028808594\n",
      "CRITIC LOSS: 104.94378662109375\n",
      "CRITIC LOSS: 100.37162780761719\n",
      "CRITIC LOSS: 104.89788055419922\n",
      "CRITIC LOSS: 94.3705062866211\n",
      "CRITIC LOSS: 97.78960418701172\n",
      "CRITIC LOSS: 97.72366333007812\n",
      "CRITIC LOSS: 98.22945404052734\n",
      "CRITIC LOSS: 93.30607604980469\n",
      "CRITIC LOSS: 101.32697296142578\n",
      "CRITIC LOSS: 93.3575210571289\n",
      "CRITIC LOSS: 91.43879699707031\n",
      "CRITIC LOSS: 98.46914672851562\n",
      "CRITIC LOSS: 91.29461669921875\n",
      "CRITIC LOSS: 81.96858215332031\n",
      "CRITIC LOSS: 83.19692993164062\n",
      "CRITIC LOSS: 84.69640350341797\n",
      "CRITIC LOSS: 87.5018539428711\n",
      "CRITIC LOSS: 84.92791748046875\n",
      "CRITIC LOSS: 81.81672668457031\n",
      "CRITIC LOSS: 76.15534210205078\n",
      "CRITIC LOSS: 74.71854400634766\n",
      "CRITIC LOSS: 94.53317260742188\n",
      "CRITIC LOSS: 83.97392272949219\n",
      "CRITIC LOSS: 96.49501037597656\n",
      "CRITIC LOSS: 88.18856048583984\n",
      "CRITIC LOSS: 115.77876281738281\n",
      "CRITIC LOSS: 79.9789810180664\n",
      "CRITIC LOSS: 80.51174926757812\n",
      "CRITIC LOSS: 71.04386138916016\n",
      "CRITIC LOSS: 77.47846984863281\n",
      "CRITIC LOSS: 85.2264633178711\n",
      "CRITIC LOSS: 71.96749877929688\n",
      "CRITIC LOSS: 78.12326049804688\n",
      "CRITIC LOSS: 78.484130859375\n",
      "CRITIC LOSS: 74.12620544433594\n",
      "CRITIC LOSS: 80.93307495117188\n",
      "CRITIC LOSS: 75.03909301757812\n",
      "CRITIC LOSS: 77.20063781738281\n",
      "CRITIC LOSS: 75.5191650390625\n",
      "CRITIC LOSS: 69.11759948730469\n",
      "CRITIC LOSS: 70.91920471191406\n",
      "CRITIC LOSS: 80.34723663330078\n",
      "CRITIC LOSS: 74.19569396972656\n",
      "CRITIC LOSS: 78.77980041503906\n",
      "CRITIC LOSS: 70.56146240234375\n",
      "CRITIC LOSS: 70.01637268066406\n",
      "CRITIC LOSS: 69.21925354003906\n",
      "CRITIC LOSS: 74.8543472290039\n",
      "CRITIC LOSS: 60.36328125\n",
      "CRITIC LOSS: 66.55696105957031\n",
      "CRITIC LOSS: 71.34705352783203\n",
      "CRITIC LOSS: 77.10621643066406\n",
      "CRITIC LOSS: 71.85183715820312\n",
      "CRITIC LOSS: 73.22943878173828\n",
      "CRITIC LOSS: 62.755306243896484\n",
      "CRITIC LOSS: 64.42640686035156\n",
      "CRITIC LOSS: 65.94781494140625\n",
      "CRITIC LOSS: 69.59786224365234\n",
      "CRITIC LOSS: 73.99293518066406\n",
      "CRITIC LOSS: 65.3192138671875\n",
      "CRITIC LOSS: 63.06547927856445\n",
      "CRITIC LOSS: 55.13151550292969\n",
      "CRITIC LOSS: 72.87605285644531\n",
      "CRITIC LOSS: 60.10812759399414\n",
      "CRITIC LOSS: 64.84896850585938\n",
      "CRITIC LOSS: 61.43018341064453\n",
      "CRITIC LOSS: 62.157501220703125\n",
      "CRITIC LOSS: 57.06590270996094\n",
      "CRITIC LOSS: 57.191802978515625\n",
      "CRITIC LOSS: 59.87335968017578\n",
      "CRITIC LOSS: 59.04506301879883\n",
      "CRITIC LOSS: 57.32029724121094\n",
      "CRITIC LOSS: 55.86042022705078\n",
      "CRITIC LOSS: 59.3885498046875\n",
      "CRITIC LOSS: 55.250282287597656\n",
      "CRITIC LOSS: 56.23057174682617\n",
      "CRITIC LOSS: 54.20632553100586\n",
      "CRITIC LOSS: 51.86882400512695\n",
      "CRITIC LOSS: 52.433876037597656\n",
      "CRITIC LOSS: 63.27845764160156\n",
      "CRITIC LOSS: 53.45468521118164\n",
      "CRITIC LOSS: 51.71669006347656\n",
      "CRITIC LOSS: 61.93889617919922\n",
      "CRITIC LOSS: 51.643707275390625\n",
      "CRITIC LOSS: 57.73495864868164\n",
      "CRITIC LOSS: 58.02018737792969\n",
      "CRITIC LOSS: 53.478607177734375\n",
      "CRITIC LOSS: 52.059783935546875\n",
      "CRITIC LOSS: 47.91472625732422\n",
      "CRITIC LOSS: 52.643978118896484\n",
      "CRITIC LOSS: 56.22746276855469\n",
      "CRITIC LOSS: 63.58699035644531\n",
      "CRITIC LOSS: 48.11438751220703\n",
      "CRITIC LOSS: 42.732967376708984\n",
      "CRITIC LOSS: 47.97212600708008\n",
      "CRITIC LOSS: 54.954471588134766\n",
      "CRITIC LOSS: 56.35715103149414\n",
      "CRITIC LOSS: 48.15000915527344\n",
      "CRITIC LOSS: 43.72114181518555\n",
      "CRITIC LOSS: 45.138492584228516\n",
      "CRITIC LOSS: 48.37165832519531\n",
      "CRITIC LOSS: 71.51676940917969\n",
      "CRITIC LOSS: 47.50846862792969\n",
      "CRITIC LOSS: 53.18506622314453\n",
      "CRITIC LOSS: 48.95819091796875\n",
      "CRITIC LOSS: 47.56956100463867\n",
      "CRITIC LOSS: 58.80014419555664\n",
      "CRITIC LOSS: 57.76704025268555\n",
      "CRITIC LOSS: 52.98095703125\n",
      "CRITIC LOSS: 48.94831085205078\n",
      "CRITIC LOSS: 46.188087463378906\n",
      "CRITIC LOSS: 47.90745162963867\n",
      "CRITIC LOSS: 39.49501419067383\n",
      "CRITIC LOSS: 42.727684020996094\n",
      "CRITIC LOSS: 46.041412353515625\n",
      "CRITIC LOSS: 43.61096954345703\n",
      "CRITIC LOSS: 43.019500732421875\n",
      "CRITIC LOSS: 46.25703430175781\n",
      "CRITIC LOSS: 44.055389404296875\n",
      "CRITIC LOSS: 41.782020568847656\n",
      "CRITIC LOSS: 38.877296447753906\n",
      "CRITIC LOSS: 40.644386291503906\n",
      "CRITIC LOSS: 36.35552215576172\n",
      "CRITIC LOSS: 38.02743148803711\n",
      "CRITIC LOSS: 45.08592224121094\n",
      "CRITIC LOSS: 43.52503967285156\n",
      "CRITIC LOSS: 36.30765151977539\n",
      "CRITIC LOSS: 45.298255920410156\n",
      "CRITIC LOSS: 42.79718017578125\n",
      "CRITIC LOSS: 41.665138244628906\n",
      "CRITIC LOSS: 45.6950569152832\n",
      "CRITIC LOSS: 36.65203857421875\n",
      "CRITIC LOSS: 36.47211456298828\n",
      "CRITIC LOSS: 44.17677307128906\n",
      "CRITIC LOSS: 37.64543151855469\n",
      "CRITIC LOSS: 39.12018966674805\n",
      "CRITIC LOSS: 39.449745178222656\n",
      "CRITIC LOSS: 36.6844482421875\n",
      "CRITIC LOSS: 40.54397964477539\n",
      "CRITIC LOSS: 37.626708984375\n",
      "CRITIC LOSS: 38.601539611816406\n",
      "CRITIC LOSS: 46.140655517578125\n",
      "CRITIC LOSS: 37.51014709472656\n",
      "CRITIC LOSS: 35.415008544921875\n",
      "CRITIC LOSS: 43.668521881103516\n",
      "CRITIC LOSS: 32.18345260620117\n",
      "CRITIC LOSS: 36.50440216064453\n",
      "CRITIC LOSS: 44.649383544921875\n",
      "CRITIC LOSS: 35.43663024902344\n",
      "CRITIC LOSS: 43.8179931640625\n",
      "CRITIC LOSS: 32.72223663330078\n",
      "CRITIC LOSS: 51.29567337036133\n",
      "CRITIC LOSS: 37.99461364746094\n",
      "CRITIC LOSS: 36.749324798583984\n",
      "CRITIC LOSS: 39.27698516845703\n",
      "CRITIC LOSS: 45.06267547607422\n",
      "CRITIC LOSS: 31.633663177490234\n",
      "CRITIC LOSS: 31.60098648071289\n",
      "CRITIC LOSS: 35.80340576171875\n",
      "CRITIC LOSS: 28.30579376220703\n",
      "CRITIC LOSS: 31.17061996459961\n",
      "CRITIC LOSS: 36.424861907958984\n",
      "CRITIC LOSS: 36.98303985595703\n",
      "CRITIC LOSS: 44.76047897338867\n",
      "CRITIC LOSS: 30.95018196105957\n",
      "CRITIC LOSS: 32.07917022705078\n",
      "CRITIC LOSS: 37.302093505859375\n",
      "CRITIC LOSS: 31.275829315185547\n",
      "CRITIC LOSS: 41.14879608154297\n",
      "CRITIC LOSS: 36.21908187866211\n",
      "CRITIC LOSS: 34.151084899902344\n",
      "CRITIC LOSS: 27.313201904296875\n",
      "CRITIC LOSS: 28.395750045776367\n",
      "CRITIC LOSS: 30.00030517578125\n",
      "CRITIC LOSS: 38.49698257446289\n",
      "CRITIC LOSS: 32.85368728637695\n",
      "CRITIC LOSS: 26.560026168823242\n",
      "CRITIC LOSS: 28.498743057250977\n",
      "CRITIC LOSS: 25.854209899902344\n",
      "CRITIC LOSS: 24.907623291015625\n",
      "CRITIC LOSS: 32.44319534301758\n",
      "CRITIC LOSS: 29.049951553344727\n",
      "CRITIC LOSS: 30.132286071777344\n",
      "CRITIC LOSS: 24.809818267822266\n",
      "CRITIC LOSS: 25.072330474853516\n",
      "CRITIC LOSS: 26.85617446899414\n",
      "CRITIC LOSS: 23.67764663696289\n",
      "CRITIC LOSS: 23.73483657836914\n",
      "CRITIC LOSS: 22.074283599853516\n",
      "CRITIC LOSS: 25.835857391357422\n",
      "CRITIC LOSS: 22.608150482177734\n",
      "CRITIC LOSS: 25.69603157043457\n",
      "CRITIC LOSS: 23.58954620361328\n",
      "CRITIC LOSS: 25.20655632019043\n",
      "CRITIC LOSS: 26.87986183166504\n",
      "CRITIC LOSS: 24.706253051757812\n",
      "CRITIC LOSS: 21.013307571411133\n",
      "CRITIC LOSS: 26.750141143798828\n",
      "CRITIC LOSS: 25.11412811279297\n",
      "CRITIC LOSS: 20.47723388671875\n",
      "CRITIC LOSS: 26.752025604248047\n",
      "CRITIC LOSS: 24.61638641357422\n",
      "CRITIC LOSS: 24.997119903564453\n",
      "CRITIC LOSS: 27.208290100097656\n",
      "CRITIC LOSS: 27.70763397216797\n",
      "CRITIC LOSS: 20.13758659362793\n",
      "CRITIC LOSS: 27.39617156982422\n",
      "CRITIC LOSS: 24.78618049621582\n",
      "CRITIC LOSS: 22.558908462524414\n",
      "CRITIC LOSS: 31.96295928955078\n",
      "CRITIC LOSS: 21.587169647216797\n",
      "CRITIC LOSS: 22.13177490234375\n",
      "CRITIC LOSS: 18.528297424316406\n",
      "CRITIC LOSS: 23.589092254638672\n",
      "CRITIC LOSS: 25.178836822509766\n",
      "CRITIC LOSS: 22.388622283935547\n",
      "CRITIC LOSS: 18.440013885498047\n",
      "CRITIC LOSS: 27.008716583251953\n",
      "CRITIC LOSS: 20.871498107910156\n",
      "CRITIC LOSS: 20.962703704833984\n",
      "CRITIC LOSS: 20.56334114074707\n",
      "CRITIC LOSS: 25.922273635864258\n",
      "CRITIC LOSS: 20.128623962402344\n",
      "CRITIC LOSS: 21.041275024414062\n",
      "CRITIC LOSS: 21.838138580322266\n",
      "CRITIC LOSS: 26.360713958740234\n",
      "CRITIC LOSS: 20.09669303894043\n",
      "CRITIC LOSS: 17.147361755371094\n",
      "CRITIC LOSS: 18.96118927001953\n",
      "CRITIC LOSS: 20.267784118652344\n",
      "CRITIC LOSS: 18.643749237060547\n",
      "CRITIC LOSS: 20.65815544128418\n",
      "CRITIC LOSS: 19.782272338867188\n",
      "CRITIC LOSS: 19.499195098876953\n",
      "CRITIC LOSS: 18.94084930419922\n",
      "CRITIC LOSS: 21.980270385742188\n",
      "CRITIC LOSS: 24.589275360107422\n",
      "CRITIC LOSS: 22.195194244384766\n",
      "CRITIC LOSS: 24.157772064208984\n",
      "CRITIC LOSS: 21.602046966552734\n",
      "CRITIC LOSS: 21.311500549316406\n",
      "CRITIC LOSS: 19.58468246459961\n",
      "CRITIC LOSS: 27.039833068847656\n",
      "CRITIC LOSS: 22.96233558654785\n",
      "CRITIC LOSS: 20.179824829101562\n",
      "CRITIC LOSS: 21.280635833740234\n",
      "CRITIC LOSS: 17.114887237548828\n",
      "CRITIC LOSS: 24.175071716308594\n",
      "CRITIC LOSS: 27.220211029052734\n",
      "CRITIC LOSS: 21.933597564697266\n",
      "CRITIC LOSS: 32.13233184814453\n",
      "CRITIC LOSS: 15.990530967712402\n",
      "CRITIC LOSS: 19.44371795654297\n",
      "CRITIC LOSS: 18.193336486816406\n",
      "CRITIC LOSS: 17.07265853881836\n",
      "CRITIC LOSS: 15.609976768493652\n",
      "CRITIC LOSS: 17.850767135620117\n",
      "CRITIC LOSS: 28.080032348632812\n",
      "CRITIC LOSS: 16.18824005126953\n",
      "CRITIC LOSS: 17.845294952392578\n",
      "CRITIC LOSS: 19.784095764160156\n",
      "CRITIC LOSS: 21.33041763305664\n",
      "CRITIC LOSS: 13.876903533935547\n",
      "CRITIC LOSS: 18.6644287109375\n",
      "CRITIC LOSS: 14.702749252319336\n",
      "CRITIC LOSS: 17.954572677612305\n",
      "CRITIC LOSS: 17.490962982177734\n",
      "CRITIC LOSS: 15.648491859436035\n",
      "CRITIC LOSS: 18.802053451538086\n",
      "CRITIC LOSS: 16.489822387695312\n",
      "CRITIC LOSS: 15.410228729248047\n",
      "CRITIC LOSS: 18.41011619567871\n",
      "CRITIC LOSS: 17.652454376220703\n",
      "CRITIC LOSS: 13.794719696044922\n",
      "CRITIC LOSS: 18.759647369384766\n",
      "CRITIC LOSS: 18.930795669555664\n",
      "CRITIC LOSS: 13.842988967895508\n",
      "CRITIC LOSS: 13.833349227905273\n",
      "CRITIC LOSS: 12.605870246887207\n",
      "CRITIC LOSS: 17.520694732666016\n",
      "CRITIC LOSS: 13.798151016235352\n",
      "CRITIC LOSS: 16.33481216430664\n",
      "CRITIC LOSS: 14.06809139251709\n",
      "CRITIC LOSS: 13.589922904968262\n",
      "CRITIC LOSS: 13.383659362792969\n",
      "CRITIC LOSS: 17.067914962768555\n",
      "CRITIC LOSS: 16.186838150024414\n",
      "CRITIC LOSS: 19.13713836669922\n",
      "CRITIC LOSS: 12.62588882446289\n",
      "CRITIC LOSS: 11.65828800201416\n",
      "CRITIC LOSS: 11.211408615112305\n",
      "CRITIC LOSS: 13.85319995880127\n",
      "CRITIC LOSS: 12.240021705627441\n",
      "CRITIC LOSS: 15.291207313537598\n",
      "CRITIC LOSS: 14.656463623046875\n",
      "CRITIC LOSS: 13.279372215270996\n",
      "CRITIC LOSS: 10.709184646606445\n",
      "CRITIC LOSS: 12.123367309570312\n",
      "CRITIC LOSS: 13.738423347473145\n",
      "CRITIC LOSS: 11.950690269470215\n",
      "CRITIC LOSS: 11.230819702148438\n",
      "CRITIC LOSS: 12.671380043029785\n",
      "CRITIC LOSS: 10.578754425048828\n",
      "CRITIC LOSS: 10.482217788696289\n",
      "CRITIC LOSS: 11.281249046325684\n",
      "CRITIC LOSS: 11.145082473754883\n",
      "CRITIC LOSS: 13.315046310424805\n",
      "CRITIC LOSS: 10.584847450256348\n",
      "CRITIC LOSS: 11.538899421691895\n",
      "CRITIC LOSS: 11.316427230834961\n",
      "CRITIC LOSS: 12.102426528930664\n",
      "CRITIC LOSS: 11.036137580871582\n",
      "CRITIC LOSS: 13.638351440429688\n",
      "CRITIC LOSS: 13.052587509155273\n",
      "CRITIC LOSS: 9.106155395507812\n",
      "-6.417093758957906\n",
      "CRITIC LOSS: 10.683157920837402\n",
      "CRITIC LOSS: 9.7277250289917\n",
      "CRITIC LOSS: 8.953964233398438\n",
      "CRITIC LOSS: 9.337677001953125\n",
      "CRITIC LOSS: 9.114082336425781\n",
      "CRITIC LOSS: 9.371456146240234\n",
      "CRITIC LOSS: 9.23668098449707\n",
      "CRITIC LOSS: 9.726859092712402\n",
      "CRITIC LOSS: 15.568428993225098\n",
      "CRITIC LOSS: 10.339539527893066\n",
      "CRITIC LOSS: 10.652578353881836\n",
      "CRITIC LOSS: 10.392666816711426\n",
      "CRITIC LOSS: 10.397848129272461\n",
      "CRITIC LOSS: 9.525440216064453\n",
      "CRITIC LOSS: 8.944908142089844\n",
      "CRITIC LOSS: 10.611434936523438\n",
      "CRITIC LOSS: 10.493896484375\n",
      "CRITIC LOSS: 11.56953239440918\n",
      "CRITIC LOSS: 9.996095657348633\n",
      "CRITIC LOSS: 10.495790481567383\n",
      "CRITIC LOSS: 10.534698486328125\n",
      "CRITIC LOSS: 7.304156303405762\n",
      "CRITIC LOSS: 11.98115062713623\n",
      "CRITIC LOSS: 10.287362098693848\n",
      "CRITIC LOSS: 9.486536979675293\n",
      "CRITIC LOSS: 10.23129653930664\n",
      "CRITIC LOSS: 14.71971321105957\n",
      "CRITIC LOSS: 10.960149765014648\n",
      "CRITIC LOSS: 10.9601469039917\n",
      "CRITIC LOSS: 15.80844497680664\n",
      "CRITIC LOSS: 10.277314186096191\n",
      "CRITIC LOSS: 7.758194446563721\n",
      "CRITIC LOSS: 10.955358505249023\n",
      "CRITIC LOSS: 10.284683227539062\n",
      "CRITIC LOSS: 8.91445255279541\n",
      "CRITIC LOSS: 8.247076034545898\n",
      "CRITIC LOSS: 8.465354919433594\n",
      "CRITIC LOSS: 7.540980339050293\n",
      "CRITIC LOSS: 8.116684913635254\n",
      "CRITIC LOSS: 8.325181007385254\n",
      "CRITIC LOSS: 7.332615852355957\n",
      "CRITIC LOSS: 8.663049697875977\n",
      "CRITIC LOSS: 8.404064178466797\n",
      "CRITIC LOSS: 11.612865447998047\n",
      "CRITIC LOSS: 7.732813835144043\n",
      "CRITIC LOSS: 7.58133602142334\n",
      "CRITIC LOSS: 7.607851028442383\n",
      "CRITIC LOSS: 9.242399215698242\n",
      "CRITIC LOSS: 6.947878837585449\n",
      "CRITIC LOSS: 8.452630996704102\n",
      "CRITIC LOSS: 9.318425178527832\n",
      "CRITIC LOSS: 6.876278400421143\n",
      "CRITIC LOSS: 6.646114349365234\n",
      "CRITIC LOSS: 7.404736042022705\n",
      "CRITIC LOSS: 7.998500823974609\n",
      "CRITIC LOSS: 9.457174301147461\n",
      "CRITIC LOSS: 6.803579330444336\n",
      "CRITIC LOSS: 7.998458385467529\n",
      "CRITIC LOSS: 6.590209007263184\n",
      "CRITIC LOSS: 9.814088821411133\n",
      "CRITIC LOSS: 6.444975852966309\n",
      "CRITIC LOSS: 13.540346145629883\n",
      "CRITIC LOSS: 7.721508026123047\n",
      "CRITIC LOSS: 8.424174308776855\n",
      "CRITIC LOSS: 6.389759063720703\n",
      "CRITIC LOSS: 8.54305648803711\n",
      "CRITIC LOSS: 6.538727760314941\n",
      "CRITIC LOSS: 7.762685298919678\n",
      "CRITIC LOSS: 7.956380844116211\n",
      "CRITIC LOSS: 9.1181640625\n",
      "CRITIC LOSS: 5.834390640258789\n",
      "CRITIC LOSS: 6.0209197998046875\n",
      "CRITIC LOSS: 6.590092658996582\n",
      "CRITIC LOSS: 7.028097152709961\n",
      "CRITIC LOSS: 6.383401870727539\n",
      "CRITIC LOSS: 7.902271747589111\n",
      "CRITIC LOSS: 5.5696258544921875\n",
      "CRITIC LOSS: 5.160594463348389\n",
      "CRITIC LOSS: 5.797792434692383\n",
      "CRITIC LOSS: 5.539298057556152\n",
      "CRITIC LOSS: 5.871732234954834\n",
      "CRITIC LOSS: 6.748688220977783\n",
      "CRITIC LOSS: 6.503602027893066\n",
      "CRITIC LOSS: 8.64447021484375\n",
      "CRITIC LOSS: 7.048277378082275\n",
      "CRITIC LOSS: 8.686504364013672\n",
      "CRITIC LOSS: 6.682930946350098\n",
      "CRITIC LOSS: 8.502242088317871\n",
      "CRITIC LOSS: 5.1853461265563965\n",
      "CRITIC LOSS: 5.448646068572998\n",
      "CRITIC LOSS: 7.552826404571533\n",
      "CRITIC LOSS: 7.682435035705566\n",
      "CRITIC LOSS: 7.850643157958984\n",
      "CRITIC LOSS: 6.582849502563477\n",
      "CRITIC LOSS: 6.25086784362793\n",
      "CRITIC LOSS: 6.479457855224609\n",
      "CRITIC LOSS: 6.3961334228515625\n",
      "CRITIC LOSS: 6.689939975738525\n",
      "CRITIC LOSS: 7.293306350708008\n",
      "CRITIC LOSS: 5.055475234985352\n",
      "CRITIC LOSS: 4.752618312835693\n",
      "CRITIC LOSS: 5.152213096618652\n",
      "CRITIC LOSS: 4.885612487792969\n",
      "CRITIC LOSS: 7.115761756896973\n",
      "CRITIC LOSS: 5.470961570739746\n",
      "CRITIC LOSS: 5.813138008117676\n",
      "CRITIC LOSS: 5.8971848487854\n",
      "CRITIC LOSS: 5.899229526519775\n",
      "CRITIC LOSS: 6.746706962585449\n",
      "CRITIC LOSS: 5.246175765991211\n",
      "CRITIC LOSS: 8.546182632446289\n",
      "CRITIC LOSS: 5.306074142456055\n",
      "CRITIC LOSS: 5.002941131591797\n",
      "CRITIC LOSS: 5.327887535095215\n",
      "CRITIC LOSS: 7.040061950683594\n",
      "CRITIC LOSS: 5.7665252685546875\n",
      "CRITIC LOSS: 5.911480903625488\n",
      "CRITIC LOSS: 4.374616622924805\n",
      "CRITIC LOSS: 5.755800247192383\n",
      "CRITIC LOSS: 6.079784393310547\n",
      "CRITIC LOSS: 5.772961616516113\n",
      "CRITIC LOSS: 5.131211280822754\n",
      "CRITIC LOSS: 5.36190938949585\n",
      "CRITIC LOSS: 5.863809585571289\n",
      "CRITIC LOSS: 6.579960823059082\n",
      "CRITIC LOSS: 6.0201568603515625\n",
      "CRITIC LOSS: 5.728507995605469\n",
      "CRITIC LOSS: 5.650506496429443\n",
      "CRITIC LOSS: 6.289078712463379\n",
      "CRITIC LOSS: 5.56269645690918\n",
      "CRITIC LOSS: 5.4699249267578125\n",
      "CRITIC LOSS: 5.201296329498291\n",
      "CRITIC LOSS: 5.541325569152832\n",
      "CRITIC LOSS: 4.305593967437744\n",
      "CRITIC LOSS: 5.828776836395264\n",
      "CRITIC LOSS: 5.434688091278076\n",
      "CRITIC LOSS: 5.049661159515381\n",
      "CRITIC LOSS: 7.042690277099609\n",
      "CRITIC LOSS: 5.332745552062988\n",
      "CRITIC LOSS: 4.785791397094727\n",
      "CRITIC LOSS: 4.549066066741943\n",
      "CRITIC LOSS: 5.634089469909668\n",
      "CRITIC LOSS: 7.849090576171875\n",
      "CRITIC LOSS: 6.74807596206665\n",
      "CRITIC LOSS: 5.593928337097168\n",
      "CRITIC LOSS: 6.240804672241211\n",
      "CRITIC LOSS: 5.514914512634277\n",
      "CRITIC LOSS: 5.8064799308776855\n",
      "CRITIC LOSS: 5.618800163269043\n",
      "CRITIC LOSS: 5.705305576324463\n",
      "CRITIC LOSS: 5.667465686798096\n",
      "CRITIC LOSS: 4.50258731842041\n",
      "CRITIC LOSS: 3.960127592086792\n",
      "CRITIC LOSS: 4.768192768096924\n",
      "CRITIC LOSS: 4.96610689163208\n",
      "CRITIC LOSS: 5.340054988861084\n",
      "CRITIC LOSS: 4.836495399475098\n",
      "CRITIC LOSS: 5.149136066436768\n",
      "CRITIC LOSS: 4.335629463195801\n",
      "CRITIC LOSS: 4.543781280517578\n",
      "CRITIC LOSS: 5.0309648513793945\n",
      "CRITIC LOSS: 4.1867828369140625\n",
      "CRITIC LOSS: 4.790327072143555\n",
      "CRITIC LOSS: 5.549211502075195\n",
      "CRITIC LOSS: 5.191563129425049\n",
      "CRITIC LOSS: 5.867902755737305\n",
      "CRITIC LOSS: 3.7204716205596924\n",
      "CRITIC LOSS: 4.76773738861084\n",
      "CRITIC LOSS: 5.529526710510254\n",
      "CRITIC LOSS: 7.238389492034912\n",
      "CRITIC LOSS: 4.941607475280762\n",
      "CRITIC LOSS: 6.316953182220459\n",
      "CRITIC LOSS: 3.783409357070923\n",
      "CRITIC LOSS: 4.992804050445557\n",
      "CRITIC LOSS: 5.100975036621094\n",
      "CRITIC LOSS: 7.1731085777282715\n",
      "CRITIC LOSS: 4.56767463684082\n",
      "CRITIC LOSS: 8.488882064819336\n",
      "CRITIC LOSS: 6.6173553466796875\n",
      "CRITIC LOSS: 6.30109977722168\n",
      "CRITIC LOSS: 4.243627548217773\n",
      "CRITIC LOSS: 10.282649040222168\n",
      "CRITIC LOSS: 7.348687171936035\n",
      "CRITIC LOSS: 6.253932952880859\n",
      "CRITIC LOSS: 6.608804702758789\n",
      "CRITIC LOSS: 4.217106819152832\n",
      "CRITIC LOSS: 4.781397819519043\n",
      "CRITIC LOSS: 4.521393775939941\n",
      "CRITIC LOSS: 7.353907585144043\n",
      "CRITIC LOSS: 6.1429877281188965\n",
      "CRITIC LOSS: 4.587887287139893\n",
      "CRITIC LOSS: 5.597729682922363\n",
      "CRITIC LOSS: 5.257040023803711\n",
      "CRITIC LOSS: 4.135561943054199\n",
      "CRITIC LOSS: 4.645328521728516\n",
      "CRITIC LOSS: 5.147243499755859\n",
      "CRITIC LOSS: 5.535751819610596\n",
      "CRITIC LOSS: 3.977438449859619\n",
      "CRITIC LOSS: 4.775431156158447\n",
      "CRITIC LOSS: 4.2430548667907715\n",
      "CRITIC LOSS: 7.064906597137451\n",
      "CRITIC LOSS: 5.5326056480407715\n",
      "CRITIC LOSS: 9.421894073486328\n",
      "CRITIC LOSS: 6.527226448059082\n",
      "CRITIC LOSS: 7.170438289642334\n",
      "CRITIC LOSS: 4.804243087768555\n",
      "CRITIC LOSS: 8.193384170532227\n",
      "CRITIC LOSS: 3.7043912410736084\n",
      "CRITIC LOSS: 3.9411144256591797\n",
      "CRITIC LOSS: 6.946534633636475\n",
      "CRITIC LOSS: 5.494635581970215\n",
      "CRITIC LOSS: 10.427677154541016\n",
      "CRITIC LOSS: 3.473494052886963\n",
      "CRITIC LOSS: 5.079873085021973\n",
      "CRITIC LOSS: 4.133337020874023\n",
      "CRITIC LOSS: 4.687261581420898\n",
      "CRITIC LOSS: 6.695262432098389\n",
      "CRITIC LOSS: 5.064825534820557\n",
      "CRITIC LOSS: 3.959528923034668\n",
      "CRITIC LOSS: 4.573129653930664\n",
      "CRITIC LOSS: 3.7615976333618164\n",
      "CRITIC LOSS: 4.541147232055664\n",
      "CRITIC LOSS: 3.24835205078125\n",
      "CRITIC LOSS: 5.422695159912109\n",
      "CRITIC LOSS: 4.381795406341553\n",
      "CRITIC LOSS: 5.309540748596191\n",
      "CRITIC LOSS: 4.5619707107543945\n",
      "CRITIC LOSS: 6.414968490600586\n",
      "CRITIC LOSS: 3.861711025238037\n",
      "CRITIC LOSS: 5.952755928039551\n",
      "CRITIC LOSS: 3.885228157043457\n",
      "CRITIC LOSS: 3.6079916954040527\n",
      "CRITIC LOSS: 3.6320905685424805\n",
      "CRITIC LOSS: 4.770547866821289\n",
      "CRITIC LOSS: 4.561592102050781\n",
      "CRITIC LOSS: 7.688366413116455\n",
      "CRITIC LOSS: 4.1801533699035645\n",
      "CRITIC LOSS: 4.914849281311035\n",
      "CRITIC LOSS: 5.778285980224609\n",
      "CRITIC LOSS: 4.028400897979736\n",
      "CRITIC LOSS: 4.62612247467041\n",
      "CRITIC LOSS: 3.8169703483581543\n",
      "CRITIC LOSS: 4.364663124084473\n",
      "CRITIC LOSS: 4.611271858215332\n",
      "CRITIC LOSS: 4.470528602600098\n",
      "CRITIC LOSS: 3.759164810180664\n",
      "CRITIC LOSS: 3.7655673027038574\n",
      "CRITIC LOSS: 3.4220409393310547\n",
      "CRITIC LOSS: 3.0407824516296387\n",
      "CRITIC LOSS: 4.455257892608643\n",
      "CRITIC LOSS: 2.688692092895508\n",
      "CRITIC LOSS: 3.7327425479888916\n",
      "CRITIC LOSS: 2.649416923522949\n",
      "CRITIC LOSS: 4.229783535003662\n",
      "CRITIC LOSS: 2.4295730590820312\n",
      "CRITIC LOSS: 3.1770286560058594\n",
      "CRITIC LOSS: 3.230231761932373\n",
      "CRITIC LOSS: 3.914679527282715\n",
      "CRITIC LOSS: 3.8628735542297363\n",
      "CRITIC LOSS: 3.1131739616394043\n",
      "CRITIC LOSS: 3.349043607711792\n",
      "CRITIC LOSS: 3.65383243560791\n",
      "CRITIC LOSS: 4.712831497192383\n",
      "CRITIC LOSS: 2.92785906791687\n",
      "CRITIC LOSS: 3.4151296615600586\n",
      "CRITIC LOSS: 3.312516689300537\n",
      "CRITIC LOSS: 3.831613063812256\n",
      "CRITIC LOSS: 2.9266347885131836\n",
      "CRITIC LOSS: 2.7032129764556885\n",
      "CRITIC LOSS: 3.8798623085021973\n",
      "CRITIC LOSS: 3.1173489093780518\n",
      "CRITIC LOSS: 9.537162780761719\n",
      "CRITIC LOSS: 3.964698553085327\n",
      "CRITIC LOSS: 3.373335361480713\n",
      "CRITIC LOSS: 6.30705451965332\n",
      "CRITIC LOSS: 3.0507073402404785\n",
      "CRITIC LOSS: 4.074009418487549\n",
      "CRITIC LOSS: 6.076834201812744\n",
      "CRITIC LOSS: 2.4514684677124023\n",
      "CRITIC LOSS: 6.924990653991699\n",
      "CRITIC LOSS: 6.066882133483887\n",
      "CRITIC LOSS: 3.3259358406066895\n",
      "CRITIC LOSS: 6.795406341552734\n",
      "CRITIC LOSS: 3.8442063331604004\n",
      "CRITIC LOSS: 4.587590217590332\n",
      "CRITIC LOSS: 3.7084908485412598\n",
      "CRITIC LOSS: 2.7037649154663086\n",
      "CRITIC LOSS: 4.800680160522461\n",
      "CRITIC LOSS: 3.236203670501709\n",
      "CRITIC LOSS: 3.096388578414917\n",
      "CRITIC LOSS: 3.7925360202789307\n",
      "CRITIC LOSS: 3.7182886600494385\n",
      "CRITIC LOSS: 4.432490825653076\n",
      "CRITIC LOSS: 3.008474826812744\n",
      "CRITIC LOSS: 3.312584400177002\n",
      "CRITIC LOSS: 3.802609443664551\n",
      "CRITIC LOSS: 3.148756980895996\n",
      "CRITIC LOSS: 3.527721643447876\n",
      "CRITIC LOSS: 2.997880458831787\n",
      "CRITIC LOSS: 4.027498245239258\n",
      "CRITIC LOSS: 3.3596198558807373\n",
      "CRITIC LOSS: 3.2259130477905273\n",
      "CRITIC LOSS: 3.007488489151001\n",
      "CRITIC LOSS: 3.381592273712158\n",
      "CRITIC LOSS: 3.521989345550537\n",
      "CRITIC LOSS: 4.232334136962891\n",
      "CRITIC LOSS: 2.3321142196655273\n",
      "CRITIC LOSS: 3.906130790710449\n",
      "CRITIC LOSS: 3.4261276721954346\n",
      "CRITIC LOSS: 3.077223777770996\n",
      "CRITIC LOSS: 3.2068400382995605\n",
      "CRITIC LOSS: 5.00752067565918\n",
      "CRITIC LOSS: 3.2234725952148438\n",
      "CRITIC LOSS: 3.495584726333618\n",
      "CRITIC LOSS: 3.741520404815674\n",
      "CRITIC LOSS: 3.001004219055176\n",
      "CRITIC LOSS: 4.471019268035889\n",
      "CRITIC LOSS: 4.109800815582275\n",
      "CRITIC LOSS: 3.0043983459472656\n",
      "CRITIC LOSS: 4.289889812469482\n",
      "CRITIC LOSS: 5.560855865478516\n",
      "CRITIC LOSS: 3.857361078262329\n",
      "CRITIC LOSS: 7.310995101928711\n",
      "CRITIC LOSS: 4.515848636627197\n",
      "CRITIC LOSS: 5.700832366943359\n",
      "CRITIC LOSS: 2.5821239948272705\n",
      "CRITIC LOSS: 6.519285202026367\n",
      "CRITIC LOSS: 4.350747585296631\n",
      "CRITIC LOSS: 3.964808464050293\n",
      "CRITIC LOSS: 5.248149871826172\n",
      "CRITIC LOSS: 2.4215712547302246\n",
      "CRITIC LOSS: 5.844611167907715\n",
      "CRITIC LOSS: 2.934225082397461\n",
      "CRITIC LOSS: 2.563575267791748\n",
      "CRITIC LOSS: 6.7727131843566895\n",
      "CRITIC LOSS: 5.246155738830566\n",
      "CRITIC LOSS: 5.8101396560668945\n",
      "CRITIC LOSS: 6.364890098571777\n",
      "CRITIC LOSS: 4.991152763366699\n",
      "CRITIC LOSS: 4.237234592437744\n",
      "CRITIC LOSS: 4.469195365905762\n",
      "CRITIC LOSS: 3.986896514892578\n",
      "CRITIC LOSS: 4.267697334289551\n",
      "CRITIC LOSS: 3.155019998550415\n",
      "CRITIC LOSS: 4.017489433288574\n",
      "CRITIC LOSS: 4.062193870544434\n",
      "CRITIC LOSS: 5.550366401672363\n",
      "CRITIC LOSS: 2.7753195762634277\n",
      "CRITIC LOSS: 7.117610454559326\n",
      "CRITIC LOSS: 2.6540110111236572\n",
      "CRITIC LOSS: 4.679620265960693\n",
      "CRITIC LOSS: 4.152866363525391\n",
      "CRITIC LOSS: 5.623805522918701\n",
      "CRITIC LOSS: 5.980440139770508\n",
      "CRITIC LOSS: 3.711495876312256\n",
      "CRITIC LOSS: 4.769584655761719\n",
      "CRITIC LOSS: 3.1138689517974854\n",
      "CRITIC LOSS: 5.0352396965026855\n",
      "CRITIC LOSS: 3.808875322341919\n",
      "CRITIC LOSS: 4.6597137451171875\n",
      "CRITIC LOSS: 3.3774051666259766\n",
      "CRITIC LOSS: 3.3476219177246094\n",
      "CRITIC LOSS: 5.003666400909424\n",
      "CRITIC LOSS: 3.089200973510742\n",
      "CRITIC LOSS: 5.827131271362305\n",
      "CRITIC LOSS: 2.8480734825134277\n",
      "CRITIC LOSS: 4.211169719696045\n",
      "CRITIC LOSS: 2.5243239402770996\n",
      "CRITIC LOSS: 3.811397075653076\n",
      "CRITIC LOSS: 2.0985617637634277\n",
      "CRITIC LOSS: 2.7559337615966797\n",
      "CRITIC LOSS: 3.2882604598999023\n",
      "CRITIC LOSS: 2.735482692718506\n",
      "CRITIC LOSS: 2.594001054763794\n",
      "CRITIC LOSS: 3.060621976852417\n",
      "CRITIC LOSS: 2.452094554901123\n",
      "CRITIC LOSS: 2.872941255569458\n",
      "CRITIC LOSS: 3.464521646499634\n",
      "CRITIC LOSS: 2.3304829597473145\n",
      "CRITIC LOSS: 3.150545120239258\n",
      "CRITIC LOSS: 2.9976398944854736\n",
      "CRITIC LOSS: 3.3063547611236572\n",
      "CRITIC LOSS: 2.621793270111084\n",
      "CRITIC LOSS: 3.214463949203491\n",
      "CRITIC LOSS: 3.5480146408081055\n",
      "CRITIC LOSS: 3.7616477012634277\n",
      "CRITIC LOSS: 3.2208166122436523\n",
      "CRITIC LOSS: 2.744919776916504\n",
      "CRITIC LOSS: 2.2969110012054443\n",
      "CRITIC LOSS: 2.3333802223205566\n",
      "CRITIC LOSS: 2.5008180141448975\n",
      "CRITIC LOSS: 2.4026107788085938\n",
      "CRITIC LOSS: 1.7718005180358887\n",
      "CRITIC LOSS: 2.6497044563293457\n",
      "CRITIC LOSS: 2.2629427909851074\n",
      "CRITIC LOSS: 2.6884496212005615\n",
      "CRITIC LOSS: 2.7668380737304688\n",
      "CRITIC LOSS: 2.434749126434326\n",
      "CRITIC LOSS: 1.9927265644073486\n",
      "CRITIC LOSS: 4.557706832885742\n",
      "CRITIC LOSS: 2.1459202766418457\n",
      "CRITIC LOSS: 2.3791332244873047\n",
      "CRITIC LOSS: 3.48238205909729\n",
      "CRITIC LOSS: 4.881451606750488\n",
      "CRITIC LOSS: 3.8154702186584473\n",
      "CRITIC LOSS: 5.488496780395508\n",
      "CRITIC LOSS: 2.3710484504699707\n",
      "CRITIC LOSS: 4.016923904418945\n",
      "CRITIC LOSS: 3.4115748405456543\n",
      "CRITIC LOSS: 3.415163278579712\n",
      "CRITIC LOSS: 2.470655679702759\n",
      "CRITIC LOSS: 2.9029788970947266\n",
      "CRITIC LOSS: 3.4611597061157227\n",
      "CRITIC LOSS: 2.884441375732422\n",
      "CRITIC LOSS: 3.475086212158203\n",
      "CRITIC LOSS: 5.2881879806518555\n",
      "CRITIC LOSS: 3.082728385925293\n",
      "CRITIC LOSS: 2.5035738945007324\n",
      "CRITIC LOSS: 2.8365986347198486\n",
      "CRITIC LOSS: 3.149501323699951\n",
      "CRITIC LOSS: 2.5262351036071777\n",
      "CRITIC LOSS: 3.037623882293701\n",
      "CRITIC LOSS: 2.671973466873169\n",
      "CRITIC LOSS: 3.1363413333892822\n",
      "CRITIC LOSS: 2.132373571395874\n",
      "CRITIC LOSS: 3.2158875465393066\n",
      "CRITIC LOSS: 2.7462968826293945\n",
      "CRITIC LOSS: 2.548694610595703\n",
      "CRITIC LOSS: 2.6927084922790527\n",
      "CRITIC LOSS: 2.435612201690674\n",
      "CRITIC LOSS: 2.3080391883850098\n",
      "CRITIC LOSS: 2.257819175720215\n",
      "CRITIC LOSS: 2.465104818344116\n",
      "CRITIC LOSS: 3.439096689224243\n",
      "CRITIC LOSS: 1.9812309741973877\n",
      "CRITIC LOSS: 2.5944695472717285\n",
      "CRITIC LOSS: 2.8323235511779785\n",
      "CRITIC LOSS: 2.734951972961426\n",
      "CRITIC LOSS: 3.2239153385162354\n",
      "CRITIC LOSS: 4.786020278930664\n",
      "CRITIC LOSS: 2.0921573638916016\n",
      "CRITIC LOSS: 2.4346165657043457\n",
      "CRITIC LOSS: 3.2446868419647217\n",
      "CRITIC LOSS: 2.430025577545166\n",
      "CRITIC LOSS: 2.8499503135681152\n",
      "CRITIC LOSS: 2.740424156188965\n",
      "CRITIC LOSS: 2.994971752166748\n",
      "CRITIC LOSS: 2.3381080627441406\n",
      "CRITIC LOSS: 2.3314197063446045\n",
      "CRITIC LOSS: 4.75974178314209\n",
      "CRITIC LOSS: 2.007244110107422\n",
      "CRITIC LOSS: 3.387160301208496\n",
      "CRITIC LOSS: 1.8509141206741333\n",
      "CRITIC LOSS: 3.6525964736938477\n",
      "CRITIC LOSS: 1.5694537162780762\n",
      "CRITIC LOSS: 5.325732231140137\n",
      "CRITIC LOSS: 2.2730093002319336\n",
      "CRITIC LOSS: 3.1100950241088867\n",
      "CRITIC LOSS: 2.2789041996002197\n",
      "CRITIC LOSS: 2.2466936111450195\n",
      "CRITIC LOSS: 3.0969390869140625\n",
      "CRITIC LOSS: 2.753664493560791\n",
      "CRITIC LOSS: 2.73677659034729\n",
      "CRITIC LOSS: 1.922499179840088\n",
      "CRITIC LOSS: 3.59745192527771\n",
      "CRITIC LOSS: 3.378390312194824\n",
      "CRITIC LOSS: 2.2879433631896973\n",
      "CRITIC LOSS: 3.977318525314331\n",
      "CRITIC LOSS: 4.571849346160889\n",
      "CRITIC LOSS: 3.6717138290405273\n",
      "CRITIC LOSS: 2.241058826446533\n",
      "CRITIC LOSS: 3.393488883972168\n",
      "CRITIC LOSS: 3.1625490188598633\n",
      "CRITIC LOSS: 3.118785858154297\n",
      "CRITIC LOSS: 2.6319360733032227\n",
      "CRITIC LOSS: 1.8262619972229004\n",
      "CRITIC LOSS: 4.026302814483643\n",
      "CRITIC LOSS: 1.764190673828125\n",
      "CRITIC LOSS: 2.312143325805664\n",
      "CRITIC LOSS: 2.8365166187286377\n",
      "CRITIC LOSS: 2.634687900543213\n",
      "CRITIC LOSS: 3.2419824600219727\n",
      "CRITIC LOSS: 2.245455741882324\n",
      "CRITIC LOSS: 2.6536471843719482\n",
      "CRITIC LOSS: 2.9096031188964844\n",
      "CRITIC LOSS: 2.2442238330841064\n",
      "CRITIC LOSS: 4.0659990310668945\n",
      "CRITIC LOSS: 2.457777976989746\n",
      "CRITIC LOSS: 3.4297213554382324\n",
      "CRITIC LOSS: 1.9460175037384033\n",
      "CRITIC LOSS: 3.701627254486084\n",
      "CRITIC LOSS: 1.9826481342315674\n",
      "CRITIC LOSS: 3.5665194988250732\n",
      "CRITIC LOSS: 2.6552963256835938\n",
      "CRITIC LOSS: 2.3248343467712402\n",
      "CRITIC LOSS: 3.1583361625671387\n",
      "CRITIC LOSS: 1.741064190864563\n",
      "CRITIC LOSS: 2.366236448287964\n",
      "CRITIC LOSS: 2.7960705757141113\n",
      "CRITIC LOSS: 2.6893959045410156\n",
      "CRITIC LOSS: 2.536797285079956\n",
      "CRITIC LOSS: 5.093060493469238\n",
      "CRITIC LOSS: 2.844830274581909\n",
      "CRITIC LOSS: 2.021286725997925\n",
      "CRITIC LOSS: 3.2379274368286133\n",
      "CRITIC LOSS: 2.204885482788086\n",
      "CRITIC LOSS: 3.054316520690918\n",
      "CRITIC LOSS: 5.139284133911133\n",
      "CRITIC LOSS: 2.3260467052459717\n",
      "CRITIC LOSS: 2.43865966796875\n",
      "CRITIC LOSS: 3.681448459625244\n",
      "CRITIC LOSS: 2.373931646347046\n",
      "CRITIC LOSS: 4.790829658508301\n",
      "CRITIC LOSS: 4.221147537231445\n",
      "CRITIC LOSS: 1.9858447313308716\n",
      "CRITIC LOSS: 2.760554790496826\n",
      "CRITIC LOSS: 3.16749906539917\n",
      "CRITIC LOSS: 2.9091968536376953\n",
      "CRITIC LOSS: 3.371857166290283\n",
      "CRITIC LOSS: 4.331954479217529\n",
      "CRITIC LOSS: 2.5618553161621094\n",
      "CRITIC LOSS: 4.037924289703369\n",
      "CRITIC LOSS: 2.1107702255249023\n",
      "CRITIC LOSS: 4.161782264709473\n",
      "CRITIC LOSS: 2.8890199661254883\n",
      "CRITIC LOSS: 4.572818756103516\n",
      "CRITIC LOSS: 2.8651013374328613\n",
      "CRITIC LOSS: 4.497098922729492\n",
      "CRITIC LOSS: 4.1920857429504395\n",
      "CRITIC LOSS: 3.0377025604248047\n",
      "CRITIC LOSS: 4.484135150909424\n",
      "CRITIC LOSS: 3.778465509414673\n",
      "CRITIC LOSS: 2.245304584503174\n",
      "CRITIC LOSS: 4.255390644073486\n",
      "CRITIC LOSS: 4.354733467102051\n",
      "CRITIC LOSS: 3.5472564697265625\n",
      "CRITIC LOSS: 2.286764621734619\n",
      "CRITIC LOSS: 2.9652886390686035\n",
      "CRITIC LOSS: 2.741602659225464\n",
      "CRITIC LOSS: 1.8071008920669556\n",
      "CRITIC LOSS: 4.35166072845459\n",
      "CRITIC LOSS: 3.3582284450531006\n",
      "CRITIC LOSS: 2.8338894844055176\n",
      "CRITIC LOSS: 4.694057941436768\n",
      "CRITIC LOSS: 2.7749125957489014\n",
      "CRITIC LOSS: 4.544501304626465\n",
      "CRITIC LOSS: 2.6159634590148926\n",
      "CRITIC LOSS: 4.7226667404174805\n",
      "CRITIC LOSS: 4.184870719909668\n",
      "CRITIC LOSS: 2.7786178588867188\n",
      "CRITIC LOSS: 3.456439733505249\n",
      "CRITIC LOSS: 2.5449512004852295\n",
      "CRITIC LOSS: 5.123763084411621\n",
      "CRITIC LOSS: 2.9690535068511963\n",
      "CRITIC LOSS: 5.20075798034668\n",
      "CRITIC LOSS: 2.6798391342163086\n",
      "CRITIC LOSS: 4.2452006340026855\n",
      "CRITIC LOSS: 2.7210426330566406\n",
      "CRITIC LOSS: 5.253786087036133\n",
      "CRITIC LOSS: 2.3781275749206543\n",
      "CRITIC LOSS: 4.207129001617432\n",
      "CRITIC LOSS: 3.2598719596862793\n",
      "CRITIC LOSS: 3.2809572219848633\n",
      "CRITIC LOSS: 3.3733692169189453\n",
      "CRITIC LOSS: 2.182039260864258\n",
      "CRITIC LOSS: 2.95955753326416\n",
      "CRITIC LOSS: 3.5944433212280273\n",
      "CRITIC LOSS: 2.8127427101135254\n",
      "CRITIC LOSS: 4.03181791305542\n",
      "CRITIC LOSS: 3.3365964889526367\n",
      "CRITIC LOSS: 3.1997170448303223\n",
      "CRITIC LOSS: 3.5389389991760254\n",
      "CRITIC LOSS: 2.241726875305176\n",
      "CRITIC LOSS: 4.475193500518799\n",
      "CRITIC LOSS: 2.234294891357422\n",
      "CRITIC LOSS: 2.7941486835479736\n",
      "CRITIC LOSS: 3.375136613845825\n",
      "CRITIC LOSS: 2.4276540279388428\n",
      "CRITIC LOSS: 2.79374361038208\n",
      "CRITIC LOSS: 2.2139039039611816\n",
      "CRITIC LOSS: 2.0994081497192383\n",
      "CRITIC LOSS: 2.0818233489990234\n",
      "CRITIC LOSS: 3.8914237022399902\n",
      "CRITIC LOSS: 2.8630757331848145\n",
      "CRITIC LOSS: 3.0145397186279297\n",
      "CRITIC LOSS: 3.2208032608032227\n",
      "CRITIC LOSS: 3.2418994903564453\n",
      "CRITIC LOSS: 2.8572778701782227\n",
      "CRITIC LOSS: 3.638068675994873\n",
      "CRITIC LOSS: 2.800218105316162\n",
      "CRITIC LOSS: 5.902010440826416\n",
      "CRITIC LOSS: 2.058154344558716\n",
      "CRITIC LOSS: 8.226974487304688\n",
      "CRITIC LOSS: 3.3453168869018555\n",
      "CRITIC LOSS: 7.484587669372559\n",
      "CRITIC LOSS: 5.029998302459717\n",
      "CRITIC LOSS: 11.000782012939453\n",
      "CRITIC LOSS: 2.8049063682556152\n",
      "CRITIC LOSS: 7.283093452453613\n",
      "CRITIC LOSS: 5.481328964233398\n",
      "CRITIC LOSS: 7.798826217651367\n",
      "CRITIC LOSS: 4.29556941986084\n",
      "CRITIC LOSS: 3.591350555419922\n",
      "CRITIC LOSS: 7.605370044708252\n",
      "CRITIC LOSS: 7.106875419616699\n",
      "CRITIC LOSS: 6.234928131103516\n",
      "CRITIC LOSS: 4.2935004234313965\n",
      "CRITIC LOSS: 6.447810649871826\n",
      "CRITIC LOSS: 2.986692428588867\n",
      "CRITIC LOSS: 6.296884536743164\n",
      "CRITIC LOSS: 3.2502312660217285\n",
      "CRITIC LOSS: 6.044973373413086\n",
      "CRITIC LOSS: 2.985896110534668\n",
      "CRITIC LOSS: 9.03011703491211\n",
      "CRITIC LOSS: 2.7852730751037598\n",
      "CRITIC LOSS: 3.5156972408294678\n",
      "CRITIC LOSS: 3.0197243690490723\n",
      "CRITIC LOSS: 3.6892623901367188\n",
      "CRITIC LOSS: 3.346858501434326\n",
      "CRITIC LOSS: 3.0902929306030273\n",
      "CRITIC LOSS: 2.7086286544799805\n",
      "CRITIC LOSS: 4.228546142578125\n",
      "CRITIC LOSS: 2.0749635696411133\n",
      "CRITIC LOSS: 3.490149736404419\n",
      "CRITIC LOSS: 3.553443431854248\n",
      "CRITIC LOSS: 3.026425838470459\n",
      "CRITIC LOSS: 2.749166965484619\n",
      "CRITIC LOSS: 3.311842679977417\n",
      "CRITIC LOSS: 3.162196636199951\n",
      "CRITIC LOSS: 2.7612051963806152\n",
      "CRITIC LOSS: 2.5246167182922363\n",
      "CRITIC LOSS: 3.819915533065796\n",
      "CRITIC LOSS: 2.477400779724121\n",
      "CRITIC LOSS: 3.1748549938201904\n",
      "CRITIC LOSS: 3.353452444076538\n",
      "CRITIC LOSS: 3.4642117023468018\n",
      "CRITIC LOSS: 3.2651946544647217\n",
      "CRITIC LOSS: 4.6952924728393555\n",
      "CRITIC LOSS: 5.4899139404296875\n",
      "CRITIC LOSS: 4.298269748687744\n",
      "CRITIC LOSS: 3.4002888202667236\n",
      "CRITIC LOSS: 4.126644134521484\n",
      "CRITIC LOSS: 3.438947916030884\n",
      "CRITIC LOSS: 7.131983280181885\n",
      "CRITIC LOSS: 4.111944198608398\n",
      "CRITIC LOSS: 5.335031032562256\n",
      "CRITIC LOSS: 2.636113166809082\n",
      "CRITIC LOSS: 4.254873275756836\n",
      "CRITIC LOSS: 2.77557110786438\n",
      "CRITIC LOSS: 3.3809561729431152\n",
      "CRITIC LOSS: 2.356292486190796\n",
      "CRITIC LOSS: 3.1954712867736816\n",
      "CRITIC LOSS: 2.1325087547302246\n",
      "CRITIC LOSS: 3.3617255687713623\n",
      "CRITIC LOSS: 3.069091320037842\n",
      "CRITIC LOSS: 2.199422836303711\n",
      "CRITIC LOSS: 3.175692081451416\n",
      "CRITIC LOSS: 3.1526055335998535\n",
      "CRITIC LOSS: 3.178093194961548\n",
      "CRITIC LOSS: 3.484802722930908\n",
      "CRITIC LOSS: 3.1131134033203125\n",
      "CRITIC LOSS: 2.35752010345459\n",
      "CRITIC LOSS: 3.857694387435913\n",
      "CRITIC LOSS: 1.9060288667678833\n",
      "CRITIC LOSS: 2.7356441020965576\n",
      "CRITIC LOSS: 2.8059942722320557\n",
      "CRITIC LOSS: 3.578840732574463\n",
      "CRITIC LOSS: 2.8530001640319824\n",
      "CRITIC LOSS: 3.2300424575805664\n",
      "CRITIC LOSS: 3.5014095306396484\n",
      "CRITIC LOSS: 4.59070348739624\n",
      "CRITIC LOSS: 2.522512912750244\n",
      "CRITIC LOSS: 3.800673484802246\n",
      "CRITIC LOSS: 2.7999672889709473\n",
      "CRITIC LOSS: 2.364800453186035\n",
      "CRITIC LOSS: 2.2408089637756348\n",
      "CRITIC LOSS: 2.566746234893799\n",
      "CRITIC LOSS: 3.800044298171997\n",
      "CRITIC LOSS: 2.170983076095581\n",
      "CRITIC LOSS: 3.342589855194092\n",
      "CRITIC LOSS: 3.003904342651367\n",
      "CRITIC LOSS: 2.501366138458252\n",
      "CRITIC LOSS: 1.9600045680999756\n",
      "CRITIC LOSS: 3.054980754852295\n",
      "CRITIC LOSS: 2.5542895793914795\n",
      "CRITIC LOSS: 2.7612245082855225\n",
      "CRITIC LOSS: 1.9467915296554565\n",
      "CRITIC LOSS: 1.7814829349517822\n",
      "CRITIC LOSS: 3.825929641723633\n",
      "CRITIC LOSS: 2.952075958251953\n",
      "CRITIC LOSS: 2.0370216369628906\n",
      "CRITIC LOSS: 2.334540367126465\n",
      "CRITIC LOSS: 1.9777675867080688\n",
      "CRITIC LOSS: 2.0746965408325195\n",
      "CRITIC LOSS: 1.9544364213943481\n",
      "CRITIC LOSS: 2.99133038520813\n",
      "CRITIC LOSS: 2.4098007678985596\n",
      "CRITIC LOSS: 4.004921913146973\n",
      "CRITIC LOSS: 2.7975597381591797\n",
      "CRITIC LOSS: 3.8677849769592285\n",
      "CRITIC LOSS: 2.551492691040039\n",
      "CRITIC LOSS: 4.168295383453369\n",
      "CRITIC LOSS: 2.36076021194458\n",
      "CRITIC LOSS: 3.210923433303833\n",
      "CRITIC LOSS: 2.748414993286133\n",
      "CRITIC LOSS: 2.50569486618042\n",
      "CRITIC LOSS: 5.799026012420654\n",
      "CRITIC LOSS: 2.55598521232605\n",
      "CRITIC LOSS: 5.81431245803833\n",
      "CRITIC LOSS: 3.605238437652588\n",
      "CRITIC LOSS: 4.393177032470703\n",
      "CRITIC LOSS: 3.57651948928833\n",
      "CRITIC LOSS: 3.979069232940674\n",
      "CRITIC LOSS: 3.7899131774902344\n",
      "CRITIC LOSS: 4.543730735778809\n",
      "CRITIC LOSS: 5.032052040100098\n",
      "CRITIC LOSS: 5.141241550445557\n",
      "CRITIC LOSS: 11.833280563354492\n",
      "CRITIC LOSS: 3.070518970489502\n",
      "CRITIC LOSS: 6.158019065856934\n",
      "CRITIC LOSS: 3.5444231033325195\n",
      "CRITIC LOSS: 4.760234832763672\n",
      "CRITIC LOSS: 5.159406661987305\n",
      "CRITIC LOSS: 9.110093116760254\n",
      "CRITIC LOSS: 7.187726020812988\n",
      "CRITIC LOSS: 4.309958457946777\n",
      "CRITIC LOSS: 12.13161849975586\n",
      "CRITIC LOSS: 17.81658935546875\n",
      "CRITIC LOSS: 5.783383369445801\n",
      "CRITIC LOSS: 13.178513526916504\n",
      "CRITIC LOSS: 25.151844024658203\n",
      "CRITIC LOSS: 14.309744834899902\n",
      "CRITIC LOSS: 15.970105171203613\n",
      "CRITIC LOSS: 7.29819393157959\n",
      "CRITIC LOSS: 13.57935619354248\n",
      "CRITIC LOSS: 15.593489646911621\n",
      "CRITIC LOSS: 9.91523551940918\n",
      "CRITIC LOSS: 10.357355117797852\n",
      "CRITIC LOSS: 11.040477752685547\n",
      "CRITIC LOSS: 11.32781982421875\n",
      "CRITIC LOSS: 13.366347312927246\n",
      "CRITIC LOSS: 10.365930557250977\n",
      "CRITIC LOSS: 19.904542922973633\n",
      "CRITIC LOSS: 14.425556182861328\n",
      "CRITIC LOSS: 12.408398628234863\n",
      "CRITIC LOSS: 11.189406394958496\n",
      "CRITIC LOSS: 12.99942398071289\n",
      "CRITIC LOSS: 6.411448001861572\n",
      "CRITIC LOSS: 9.9580659866333\n",
      "CRITIC LOSS: 13.43848705291748\n",
      "CRITIC LOSS: 9.394179344177246\n",
      "CRITIC LOSS: 15.765939712524414\n",
      "CRITIC LOSS: 12.880327224731445\n",
      "CRITIC LOSS: 7.984809875488281\n",
      "CRITIC LOSS: 9.953886032104492\n",
      "CRITIC LOSS: 7.85911750793457\n",
      "CRITIC LOSS: 8.365500450134277\n",
      "CRITIC LOSS: 10.915575981140137\n",
      "CRITIC LOSS: 9.025993347167969\n",
      "CRITIC LOSS: 4.3638153076171875\n",
      "CRITIC LOSS: 9.058075904846191\n",
      "CRITIC LOSS: 6.9423723220825195\n",
      "CRITIC LOSS: 6.479551315307617\n",
      "CRITIC LOSS: 6.376347541809082\n",
      "CRITIC LOSS: 7.048879623413086\n",
      "CRITIC LOSS: 8.645662307739258\n",
      "CRITIC LOSS: 8.011780738830566\n",
      "CRITIC LOSS: 5.697605133056641\n",
      "CRITIC LOSS: 6.4987382888793945\n",
      "CRITIC LOSS: 5.9235076904296875\n",
      "CRITIC LOSS: 5.162054061889648\n",
      "CRITIC LOSS: 7.640806198120117\n",
      "CRITIC LOSS: 5.937177658081055\n",
      "CRITIC LOSS: 8.542017936706543\n",
      "CRITIC LOSS: 3.773285388946533\n",
      "CRITIC LOSS: 7.82331657409668\n",
      "CRITIC LOSS: 4.816600799560547\n",
      "CRITIC LOSS: 6.860696792602539\n",
      "CRITIC LOSS: 7.344764709472656\n",
      "CRITIC LOSS: 4.990506172180176\n",
      "CRITIC LOSS: 6.474287033081055\n",
      "CRITIC LOSS: 3.8971972465515137\n",
      "CRITIC LOSS: 5.67353630065918\n",
      "CRITIC LOSS: 4.499053001403809\n",
      "CRITIC LOSS: 5.019950866699219\n",
      "CRITIC LOSS: 3.08449125289917\n",
      "CRITIC LOSS: 4.873752117156982\n",
      "CRITIC LOSS: 5.305519104003906\n",
      "CRITIC LOSS: 5.762164115905762\n",
      "CRITIC LOSS: 5.931614875793457\n",
      "CRITIC LOSS: 3.5150680541992188\n",
      "CRITIC LOSS: 4.821521759033203\n",
      "CRITIC LOSS: 4.47681999206543\n",
      "CRITIC LOSS: 4.705039024353027\n",
      "CRITIC LOSS: 5.698601722717285\n",
      "CRITIC LOSS: 3.5230090618133545\n",
      "CRITIC LOSS: 4.35257625579834\n",
      "CRITIC LOSS: 3.530268430709839\n",
      "CRITIC LOSS: 4.697315216064453\n",
      "CRITIC LOSS: 3.8487424850463867\n",
      "CRITIC LOSS: 2.802809953689575\n",
      "CRITIC LOSS: 2.8978817462921143\n",
      "CRITIC LOSS: 2.7550253868103027\n",
      "CRITIC LOSS: 5.5271453857421875\n",
      "CRITIC LOSS: 4.004539489746094\n",
      "CRITIC LOSS: 2.445204257965088\n",
      "CRITIC LOSS: 3.820425033569336\n",
      "CRITIC LOSS: 3.0481700897216797\n",
      "CRITIC LOSS: 3.787128448486328\n",
      "CRITIC LOSS: 2.8711061477661133\n",
      "CRITIC LOSS: 3.601186990737915\n",
      "CRITIC LOSS: 3.5455946922302246\n",
      "CRITIC LOSS: 3.373717784881592\n",
      "CRITIC LOSS: 4.568767070770264\n",
      "CRITIC LOSS: 2.4637575149536133\n",
      "CRITIC LOSS: 4.239966869354248\n",
      "CRITIC LOSS: 3.0283544063568115\n",
      "CRITIC LOSS: 4.895714282989502\n",
      "CRITIC LOSS: 3.05330753326416\n",
      "CRITIC LOSS: 4.2602338790893555\n",
      "CRITIC LOSS: 3.9400691986083984\n",
      "CRITIC LOSS: 4.870761394500732\n",
      "CRITIC LOSS: 3.2975893020629883\n",
      "CRITIC LOSS: 2.554107189178467\n",
      "CRITIC LOSS: 2.2814221382141113\n",
      "CRITIC LOSS: 3.8451883792877197\n",
      "CRITIC LOSS: 2.7747349739074707\n",
      "CRITIC LOSS: 4.091585159301758\n",
      "CRITIC LOSS: 4.328576564788818\n",
      "CRITIC LOSS: 3.5708017349243164\n",
      "CRITIC LOSS: 5.3696393966674805\n",
      "CRITIC LOSS: 3.934532403945923\n",
      "CRITIC LOSS: 3.257399797439575\n",
      "CRITIC LOSS: 2.2376840114593506\n",
      "CRITIC LOSS: 3.5342564582824707\n",
      "CRITIC LOSS: 3.7536187171936035\n",
      "CRITIC LOSS: 3.5704755783081055\n",
      "CRITIC LOSS: 2.6725821495056152\n",
      "CRITIC LOSS: 3.192235231399536\n",
      "CRITIC LOSS: 3.161245346069336\n",
      "CRITIC LOSS: 4.053839206695557\n",
      "CRITIC LOSS: 3.1809215545654297\n",
      "CRITIC LOSS: 4.6593804359436035\n",
      "CRITIC LOSS: 3.342160940170288\n",
      "CRITIC LOSS: 4.152974605560303\n",
      "CRITIC LOSS: 4.912589073181152\n",
      "CRITIC LOSS: 9.121986389160156\n",
      "CRITIC LOSS: 4.0567474365234375\n",
      "CRITIC LOSS: 8.87923812866211\n",
      "CRITIC LOSS: 5.088043689727783\n",
      "CRITIC LOSS: 10.95702838897705\n",
      "CRITIC LOSS: 4.165205955505371\n",
      "CRITIC LOSS: 5.028935432434082\n",
      "CRITIC LOSS: 4.019017219543457\n",
      "CRITIC LOSS: 4.698738098144531\n",
      "CRITIC LOSS: 5.865461349487305\n",
      "CRITIC LOSS: 4.403562545776367\n",
      "CRITIC LOSS: 6.653482437133789\n",
      "CRITIC LOSS: 3.0658297538757324\n",
      "CRITIC LOSS: 7.109928131103516\n",
      "CRITIC LOSS: 3.199401378631592\n",
      "CRITIC LOSS: 7.183870315551758\n",
      "CRITIC LOSS: 4.428379535675049\n",
      "CRITIC LOSS: 6.7304229736328125\n",
      "CRITIC LOSS: 5.9043684005737305\n",
      "CRITIC LOSS: 6.559394359588623\n",
      "CRITIC LOSS: 6.821739196777344\n",
      "CRITIC LOSS: 5.194239616394043\n",
      "CRITIC LOSS: 4.885783672332764\n",
      "CRITIC LOSS: 4.268852233886719\n",
      "CRITIC LOSS: 3.7125210762023926\n",
      "CRITIC LOSS: 4.068104267120361\n",
      "CRITIC LOSS: 3.1485538482666016\n",
      "CRITIC LOSS: 3.533456325531006\n",
      "CRITIC LOSS: 3.61985182762146\n",
      "CRITIC LOSS: 4.553158760070801\n",
      "CRITIC LOSS: 3.0403695106506348\n",
      "CRITIC LOSS: 2.727696180343628\n",
      "CRITIC LOSS: 3.947244167327881\n",
      "CRITIC LOSS: 3.1041173934936523\n",
      "CRITIC LOSS: 2.541512966156006\n",
      "CRITIC LOSS: 4.072227478027344\n",
      "CRITIC LOSS: 3.3258445262908936\n",
      "CRITIC LOSS: 4.298600196838379\n",
      "CRITIC LOSS: 3.31472110748291\n",
      "CRITIC LOSS: 2.7076330184936523\n",
      "CRITIC LOSS: 3.6051552295684814\n",
      "CRITIC LOSS: 3.530564308166504\n",
      "CRITIC LOSS: 2.9143929481506348\n",
      "CRITIC LOSS: 2.3608670234680176\n",
      "CRITIC LOSS: 3.7517900466918945\n",
      "CRITIC LOSS: 3.2238926887512207\n",
      "CRITIC LOSS: 2.2535276412963867\n",
      "CRITIC LOSS: 3.787200927734375\n",
      "CRITIC LOSS: 2.978335380554199\n",
      "CRITIC LOSS: 2.345257043838501\n",
      "CRITIC LOSS: 2.311093330383301\n",
      "CRITIC LOSS: 2.4464330673217773\n",
      "CRITIC LOSS: 2.0595407485961914\n",
      "CRITIC LOSS: 2.822906970977783\n",
      "CRITIC LOSS: 1.9203104972839355\n",
      "CRITIC LOSS: 2.840360403060913\n",
      "CRITIC LOSS: 2.469125270843506\n",
      "CRITIC LOSS: 2.737415313720703\n",
      "CRITIC LOSS: 3.108717918395996\n",
      "CRITIC LOSS: 3.078411340713501\n",
      "CRITIC LOSS: 2.1878561973571777\n",
      "CRITIC LOSS: 2.666389226913452\n",
      "CRITIC LOSS: 2.7600879669189453\n",
      "CRITIC LOSS: 4.41707706451416\n",
      "CRITIC LOSS: 1.9351294040679932\n",
      "CRITIC LOSS: 2.1146984100341797\n",
      "CRITIC LOSS: 2.671304702758789\n",
      "CRITIC LOSS: 2.2532095909118652\n",
      "CRITIC LOSS: 3.668142318725586\n",
      "CRITIC LOSS: 1.8262901306152344\n",
      "CRITIC LOSS: 2.4249191284179688\n",
      "CRITIC LOSS: 3.3454012870788574\n",
      "CRITIC LOSS: 2.7488417625427246\n",
      "CRITIC LOSS: 2.7483630180358887\n",
      "CRITIC LOSS: 2.1121127605438232\n",
      "CRITIC LOSS: 2.963040351867676\n",
      "CRITIC LOSS: 3.2182867527008057\n",
      "CRITIC LOSS: 2.7580552101135254\n",
      "CRITIC LOSS: 3.724736213684082\n",
      "CRITIC LOSS: 3.7589168548583984\n",
      "CRITIC LOSS: 2.92486572265625\n",
      "CRITIC LOSS: 3.5988385677337646\n",
      "CRITIC LOSS: 3.9026474952697754\n",
      "CRITIC LOSS: 3.3248612880706787\n",
      "CRITIC LOSS: 3.5906612873077393\n",
      "CRITIC LOSS: 5.056934833526611\n",
      "CRITIC LOSS: 3.9920759201049805\n",
      "CRITIC LOSS: 6.7463202476501465\n",
      "CRITIC LOSS: 3.233881950378418\n",
      "CRITIC LOSS: 5.805802822113037\n",
      "CRITIC LOSS: 2.532210350036621\n",
      "CRITIC LOSS: 4.91749906539917\n",
      "CRITIC LOSS: 2.9862866401672363\n",
      "CRITIC LOSS: 5.0517778396606445\n",
      "CRITIC LOSS: 5.055355548858643\n",
      "CRITIC LOSS: 5.588040351867676\n",
      "CRITIC LOSS: 4.371058464050293\n",
      "CRITIC LOSS: 3.176004409790039\n",
      "CRITIC LOSS: 3.1245551109313965\n",
      "CRITIC LOSS: 5.1700358390808105\n",
      "CRITIC LOSS: 2.1789729595184326\n",
      "CRITIC LOSS: 3.0200343132019043\n",
      "CRITIC LOSS: 2.6784510612487793\n",
      "CRITIC LOSS: 2.341235637664795\n",
      "CRITIC LOSS: 2.53090238571167\n",
      "CRITIC LOSS: 2.5402538776397705\n",
      "CRITIC LOSS: 3.2309744358062744\n",
      "CRITIC LOSS: 1.8676782846450806\n",
      "CRITIC LOSS: 3.396389961242676\n",
      "CRITIC LOSS: 2.248025417327881\n",
      "CRITIC LOSS: 2.1746182441711426\n",
      "CRITIC LOSS: 1.9982905387878418\n",
      "CRITIC LOSS: 2.327207565307617\n",
      "CRITIC LOSS: 2.5426175594329834\n",
      "CRITIC LOSS: 2.812715530395508\n",
      "CRITIC LOSS: 2.332758903503418\n",
      "CRITIC LOSS: 2.0614657402038574\n",
      "CRITIC LOSS: 2.2245562076568604\n",
      "CRITIC LOSS: 1.8820425271987915\n",
      "CRITIC LOSS: 2.08353328704834\n",
      "CRITIC LOSS: 1.9562186002731323\n",
      "CRITIC LOSS: 2.1507983207702637\n",
      "CRITIC LOSS: 1.8199374675750732\n",
      "CRITIC LOSS: 2.5356945991516113\n",
      "CRITIC LOSS: 3.117696762084961\n",
      "CRITIC LOSS: 2.1295506954193115\n",
      "CRITIC LOSS: 2.232917070388794\n",
      "CRITIC LOSS: 2.166269302368164\n",
      "CRITIC LOSS: 2.933971405029297\n",
      "CRITIC LOSS: 2.138517379760742\n",
      "CRITIC LOSS: 1.7323501110076904\n",
      "CRITIC LOSS: 1.4399446249008179\n",
      "CRITIC LOSS: 2.383840560913086\n",
      "CRITIC LOSS: 1.5929304361343384\n",
      "-6.26941440671112\n",
      "CRITIC LOSS: 2.2157516479492188\n",
      "CRITIC LOSS: 2.3648602962493896\n",
      "CRITIC LOSS: 3.3429431915283203\n",
      "CRITIC LOSS: 2.9586997032165527\n",
      "CRITIC LOSS: 4.398185729980469\n",
      "CRITIC LOSS: 3.100964069366455\n",
      "CRITIC LOSS: 3.1025588512420654\n",
      "CRITIC LOSS: 4.6699066162109375\n",
      "CRITIC LOSS: 2.93947434425354\n",
      "CRITIC LOSS: 2.3896963596343994\n",
      "CRITIC LOSS: 3.8750336170196533\n",
      "CRITIC LOSS: 2.3950061798095703\n",
      "CRITIC LOSS: 3.0188918113708496\n",
      "CRITIC LOSS: 2.462625503540039\n",
      "CRITIC LOSS: 3.1240782737731934\n",
      "CRITIC LOSS: 2.511378288269043\n",
      "CRITIC LOSS: 2.9608399868011475\n",
      "CRITIC LOSS: 2.7433321475982666\n",
      "CRITIC LOSS: 3.998237371444702\n",
      "CRITIC LOSS: 2.494356870651245\n",
      "CRITIC LOSS: 5.1923322677612305\n",
      "CRITIC LOSS: 3.034794330596924\n",
      "CRITIC LOSS: 3.2111706733703613\n",
      "CRITIC LOSS: 2.1874637603759766\n",
      "CRITIC LOSS: 2.9618330001831055\n",
      "CRITIC LOSS: 3.9207892417907715\n",
      "CRITIC LOSS: 2.6892943382263184\n",
      "CRITIC LOSS: 3.2576589584350586\n",
      "CRITIC LOSS: 3.828014373779297\n",
      "CRITIC LOSS: 3.626577377319336\n",
      "CRITIC LOSS: 3.0879759788513184\n",
      "CRITIC LOSS: 3.742478609085083\n",
      "CRITIC LOSS: 2.279141902923584\n",
      "CRITIC LOSS: 2.3358798027038574\n",
      "CRITIC LOSS: 3.839146614074707\n",
      "CRITIC LOSS: 1.808345079421997\n",
      "CRITIC LOSS: 2.6280970573425293\n",
      "CRITIC LOSS: 2.063565254211426\n",
      "CRITIC LOSS: 1.9013851881027222\n",
      "CRITIC LOSS: 2.0427024364471436\n",
      "CRITIC LOSS: 1.9196512699127197\n",
      "CRITIC LOSS: 2.5302863121032715\n",
      "CRITIC LOSS: 2.375532388687134\n",
      "CRITIC LOSS: 2.388561248779297\n",
      "CRITIC LOSS: 1.689828634262085\n",
      "CRITIC LOSS: 3.347259998321533\n",
      "CRITIC LOSS: 1.725953459739685\n",
      "CRITIC LOSS: 2.0521657466888428\n",
      "CRITIC LOSS: 2.3376717567443848\n",
      "CRITIC LOSS: 2.6034257411956787\n",
      "CRITIC LOSS: 1.4679125547409058\n",
      "CRITIC LOSS: 2.646756887435913\n",
      "CRITIC LOSS: 1.621711015701294\n",
      "CRITIC LOSS: 2.3474769592285156\n",
      "CRITIC LOSS: 1.3974716663360596\n",
      "CRITIC LOSS: 2.0213284492492676\n",
      "CRITIC LOSS: 2.287458658218384\n",
      "CRITIC LOSS: 1.7782536745071411\n",
      "CRITIC LOSS: 2.27634596824646\n",
      "CRITIC LOSS: 2.622565269470215\n",
      "CRITIC LOSS: 2.0389046669006348\n",
      "CRITIC LOSS: 3.0335466861724854\n",
      "CRITIC LOSS: 2.6797685623168945\n",
      "CRITIC LOSS: 1.3925563097000122\n",
      "CRITIC LOSS: 2.55294132232666\n",
      "CRITIC LOSS: 1.9856579303741455\n",
      "CRITIC LOSS: 1.7815871238708496\n",
      "CRITIC LOSS: 2.1964516639709473\n",
      "CRITIC LOSS: 2.012428045272827\n",
      "CRITIC LOSS: 1.7514714002609253\n",
      "CRITIC LOSS: 2.416180372238159\n",
      "CRITIC LOSS: 2.568939685821533\n",
      "CRITIC LOSS: 3.648714303970337\n",
      "CRITIC LOSS: 3.6043663024902344\n",
      "CRITIC LOSS: 2.0057928562164307\n",
      "CRITIC LOSS: 4.896575927734375\n",
      "CRITIC LOSS: 2.7726550102233887\n",
      "CRITIC LOSS: 1.9691613912582397\n",
      "CRITIC LOSS: 1.9031996726989746\n",
      "CRITIC LOSS: 2.3239905834198\n",
      "CRITIC LOSS: 2.0365428924560547\n",
      "CRITIC LOSS: 2.553257703781128\n",
      "CRITIC LOSS: 1.5843487977981567\n",
      "CRITIC LOSS: 2.4994888305664062\n",
      "CRITIC LOSS: 1.9873559474945068\n",
      "CRITIC LOSS: 3.1897683143615723\n",
      "CRITIC LOSS: 2.2768545150756836\n",
      "CRITIC LOSS: 2.8501806259155273\n",
      "CRITIC LOSS: 2.6539721488952637\n",
      "CRITIC LOSS: 1.8470704555511475\n",
      "CRITIC LOSS: 2.9356672763824463\n",
      "CRITIC LOSS: 2.2557997703552246\n",
      "CRITIC LOSS: 2.1753249168395996\n",
      "CRITIC LOSS: 2.494612216949463\n",
      "CRITIC LOSS: 2.1487884521484375\n",
      "CRITIC LOSS: 2.1446120738983154\n",
      "CRITIC LOSS: 2.0906591415405273\n",
      "CRITIC LOSS: 2.1922760009765625\n",
      "CRITIC LOSS: 2.545487880706787\n",
      "CRITIC LOSS: 4.612924575805664\n",
      "CRITIC LOSS: 1.8880442380905151\n",
      "CRITIC LOSS: 2.3620057106018066\n",
      "CRITIC LOSS: 2.339528799057007\n",
      "CRITIC LOSS: 2.7593202590942383\n",
      "CRITIC LOSS: 2.0130293369293213\n",
      "CRITIC LOSS: 2.030726909637451\n",
      "CRITIC LOSS: 1.8739835023880005\n",
      "CRITIC LOSS: 2.3596959114074707\n",
      "CRITIC LOSS: 3.335916519165039\n",
      "CRITIC LOSS: 2.3532204627990723\n",
      "CRITIC LOSS: 3.228717803955078\n",
      "CRITIC LOSS: 2.715264320373535\n",
      "CRITIC LOSS: 2.2447726726531982\n",
      "CRITIC LOSS: 3.506988048553467\n",
      "CRITIC LOSS: 1.8784101009368896\n",
      "CRITIC LOSS: 2.162661552429199\n",
      "CRITIC LOSS: 2.6551895141601562\n",
      "CRITIC LOSS: 3.1061413288116455\n",
      "CRITIC LOSS: 2.6031928062438965\n",
      "CRITIC LOSS: 2.420496940612793\n",
      "CRITIC LOSS: 2.898493766784668\n",
      "CRITIC LOSS: 2.869093418121338\n",
      "CRITIC LOSS: 1.7380423545837402\n",
      "CRITIC LOSS: 2.4243204593658447\n",
      "CRITIC LOSS: 1.6874443292617798\n",
      "CRITIC LOSS: 3.0085229873657227\n",
      "CRITIC LOSS: 2.37172269821167\n",
      "CRITIC LOSS: 2.712578058242798\n",
      "CRITIC LOSS: 1.9967842102050781\n",
      "CRITIC LOSS: 2.0755743980407715\n",
      "CRITIC LOSS: 1.9212234020233154\n",
      "CRITIC LOSS: 2.0329291820526123\n",
      "CRITIC LOSS: 2.7840423583984375\n",
      "CRITIC LOSS: 2.429189682006836\n",
      "CRITIC LOSS: 4.203906059265137\n",
      "CRITIC LOSS: 2.179605007171631\n",
      "CRITIC LOSS: 2.6707663536071777\n",
      "CRITIC LOSS: 1.3595023155212402\n",
      "CRITIC LOSS: 2.8478734493255615\n",
      "CRITIC LOSS: 1.9207165241241455\n",
      "CRITIC LOSS: 3.053199529647827\n",
      "CRITIC LOSS: 1.8165005445480347\n",
      "CRITIC LOSS: 2.9814724922180176\n",
      "CRITIC LOSS: 2.3581490516662598\n",
      "CRITIC LOSS: 3.1516213417053223\n",
      "CRITIC LOSS: 1.9904136657714844\n",
      "CRITIC LOSS: 2.4515540599823\n",
      "CRITIC LOSS: 3.047323226928711\n",
      "CRITIC LOSS: 2.7837419509887695\n",
      "CRITIC LOSS: 3.2631449699401855\n",
      "CRITIC LOSS: 2.3366575241088867\n",
      "CRITIC LOSS: 2.7343318462371826\n",
      "CRITIC LOSS: 3.735074520111084\n",
      "CRITIC LOSS: 2.5411345958709717\n",
      "CRITIC LOSS: 6.288722038269043\n",
      "CRITIC LOSS: 4.069441795349121\n",
      "CRITIC LOSS: 4.024497985839844\n",
      "CRITIC LOSS: 4.6733832359313965\n",
      "CRITIC LOSS: 7.600169658660889\n",
      "CRITIC LOSS: 3.849841356277466\n",
      "CRITIC LOSS: 9.248876571655273\n",
      "CRITIC LOSS: 4.142806053161621\n",
      "CRITIC LOSS: 3.493232011795044\n",
      "CRITIC LOSS: 4.708906650543213\n",
      "CRITIC LOSS: 3.6296939849853516\n",
      "CRITIC LOSS: 5.063664436340332\n",
      "CRITIC LOSS: 3.6871449947357178\n",
      "CRITIC LOSS: 3.9881348609924316\n",
      "CRITIC LOSS: 3.31510066986084\n",
      "CRITIC LOSS: 5.213223934173584\n",
      "CRITIC LOSS: 2.874480724334717\n",
      "CRITIC LOSS: 3.9323062896728516\n",
      "CRITIC LOSS: 5.367922306060791\n",
      "CRITIC LOSS: 5.159209251403809\n",
      "CRITIC LOSS: 4.666733741760254\n",
      "CRITIC LOSS: 4.652863502502441\n",
      "CRITIC LOSS: 2.8813281059265137\n",
      "CRITIC LOSS: 5.032130241394043\n",
      "CRITIC LOSS: 2.236398696899414\n",
      "CRITIC LOSS: 2.6880037784576416\n",
      "CRITIC LOSS: 4.234251499176025\n",
      "CRITIC LOSS: 2.849012851715088\n",
      "CRITIC LOSS: 2.822117805480957\n",
      "CRITIC LOSS: 2.277444839477539\n",
      "CRITIC LOSS: 2.2807087898254395\n",
      "CRITIC LOSS: 3.332901954650879\n",
      "CRITIC LOSS: 2.21761155128479\n",
      "CRITIC LOSS: 1.7265533208847046\n",
      "CRITIC LOSS: 2.531791925430298\n",
      "CRITIC LOSS: 1.7680248022079468\n",
      "CRITIC LOSS: 2.5237152576446533\n",
      "CRITIC LOSS: 2.956235647201538\n",
      "CRITIC LOSS: 1.8109170198440552\n",
      "CRITIC LOSS: 3.5519607067108154\n",
      "CRITIC LOSS: 2.333615779876709\n",
      "CRITIC LOSS: 2.9056236743927\n",
      "CRITIC LOSS: 1.741119623184204\n",
      "CRITIC LOSS: 3.177727699279785\n",
      "CRITIC LOSS: 2.7230982780456543\n",
      "CRITIC LOSS: 3.366184711456299\n",
      "CRITIC LOSS: 2.500999927520752\n",
      "CRITIC LOSS: 2.3400731086730957\n",
      "CRITIC LOSS: 3.2578325271606445\n",
      "CRITIC LOSS: 2.2107882499694824\n",
      "CRITIC LOSS: 2.678759813308716\n",
      "CRITIC LOSS: 2.260315418243408\n",
      "CRITIC LOSS: 2.308987617492676\n",
      "CRITIC LOSS: 2.97945237159729\n",
      "CRITIC LOSS: 2.520881175994873\n",
      "CRITIC LOSS: 2.4561986923217773\n",
      "CRITIC LOSS: 2.1318836212158203\n",
      "CRITIC LOSS: 2.9430670738220215\n",
      "CRITIC LOSS: 3.508091688156128\n",
      "CRITIC LOSS: 4.739664554595947\n",
      "CRITIC LOSS: 2.236027240753174\n",
      "CRITIC LOSS: 3.755127429962158\n",
      "CRITIC LOSS: 4.755153656005859\n",
      "CRITIC LOSS: 2.7732794284820557\n",
      "CRITIC LOSS: 5.124562740325928\n",
      "CRITIC LOSS: 1.8842717409133911\n",
      "CRITIC LOSS: 2.884065628051758\n",
      "CRITIC LOSS: 3.385331153869629\n",
      "CRITIC LOSS: 2.8611912727355957\n",
      "CRITIC LOSS: 3.6797969341278076\n",
      "CRITIC LOSS: 2.9640588760375977\n",
      "CRITIC LOSS: 3.433138370513916\n",
      "CRITIC LOSS: 4.146755218505859\n",
      "CRITIC LOSS: 1.837626338005066\n",
      "CRITIC LOSS: 3.5982577800750732\n",
      "CRITIC LOSS: 2.414980888366699\n",
      "CRITIC LOSS: 2.700756549835205\n",
      "CRITIC LOSS: 2.462690830230713\n",
      "CRITIC LOSS: 2.2402753829956055\n",
      "CRITIC LOSS: 2.1763546466827393\n",
      "CRITIC LOSS: 3.394035577774048\n",
      "CRITIC LOSS: 1.6379287242889404\n",
      "CRITIC LOSS: 3.888853073120117\n",
      "CRITIC LOSS: 2.5946569442749023\n",
      "CRITIC LOSS: 4.239760398864746\n",
      "CRITIC LOSS: 1.9956133365631104\n",
      "CRITIC LOSS: 3.8466272354125977\n",
      "CRITIC LOSS: 2.8364291191101074\n",
      "CRITIC LOSS: 3.080822467803955\n",
      "CRITIC LOSS: 3.1269593238830566\n",
      "CRITIC LOSS: 2.8028597831726074\n",
      "CRITIC LOSS: 3.4588630199432373\n",
      "CRITIC LOSS: 3.8528218269348145\n",
      "CRITIC LOSS: 3.6649169921875\n",
      "CRITIC LOSS: 2.834381103515625\n",
      "CRITIC LOSS: 3.1164746284484863\n",
      "CRITIC LOSS: 3.3132052421569824\n",
      "CRITIC LOSS: 3.7270255088806152\n",
      "CRITIC LOSS: 1.7683658599853516\n",
      "CRITIC LOSS: 3.563528537750244\n",
      "CRITIC LOSS: 2.4812276363372803\n",
      "CRITIC LOSS: 2.7762656211853027\n",
      "CRITIC LOSS: 1.2909619808197021\n",
      "CRITIC LOSS: 3.5729846954345703\n",
      "CRITIC LOSS: 2.5583205223083496\n",
      "CRITIC LOSS: 2.319066286087036\n",
      "CRITIC LOSS: 2.1307313442230225\n",
      "CRITIC LOSS: 2.1309876441955566\n",
      "CRITIC LOSS: 1.664255976676941\n",
      "CRITIC LOSS: 2.918673276901245\n",
      "CRITIC LOSS: 2.4741532802581787\n",
      "CRITIC LOSS: 2.0384206771850586\n",
      "CRITIC LOSS: 1.975328803062439\n",
      "CRITIC LOSS: 3.3169028759002686\n",
      "CRITIC LOSS: 2.397190570831299\n",
      "CRITIC LOSS: 2.5573208332061768\n",
      "CRITIC LOSS: 1.3899049758911133\n",
      "CRITIC LOSS: 3.381948471069336\n",
      "CRITIC LOSS: 1.7573738098144531\n",
      "CRITIC LOSS: 2.1628036499023438\n",
      "CRITIC LOSS: 1.9131813049316406\n",
      "CRITIC LOSS: 2.5555191040039062\n",
      "CRITIC LOSS: 1.7937910556793213\n",
      "CRITIC LOSS: 3.0951409339904785\n",
      "CRITIC LOSS: 2.1051559448242188\n",
      "CRITIC LOSS: 1.7934095859527588\n",
      "CRITIC LOSS: 2.437361240386963\n",
      "CRITIC LOSS: 2.1796765327453613\n",
      "CRITIC LOSS: 3.3181354999542236\n",
      "CRITIC LOSS: 1.3466393947601318\n",
      "CRITIC LOSS: 1.501204013824463\n",
      "CRITIC LOSS: 1.854966640472412\n",
      "CRITIC LOSS: 1.4921760559082031\n",
      "CRITIC LOSS: 3.614044189453125\n",
      "CRITIC LOSS: 1.8195033073425293\n",
      "CRITIC LOSS: 2.430805206298828\n",
      "CRITIC LOSS: 2.1641016006469727\n",
      "CRITIC LOSS: 2.6928892135620117\n",
      "CRITIC LOSS: 2.5876002311706543\n",
      "CRITIC LOSS: 1.986035704612732\n",
      "CRITIC LOSS: 1.8786109685897827\n",
      "CRITIC LOSS: 2.7606372833251953\n",
      "CRITIC LOSS: 3.9779276847839355\n",
      "CRITIC LOSS: 2.3352713584899902\n",
      "CRITIC LOSS: 1.6742026805877686\n",
      "CRITIC LOSS: 2.1513547897338867\n",
      "CRITIC LOSS: 1.9874260425567627\n",
      "CRITIC LOSS: 1.5693359375\n",
      "CRITIC LOSS: 2.7582309246063232\n",
      "CRITIC LOSS: 2.0111522674560547\n",
      "CRITIC LOSS: 1.9967395067214966\n",
      "CRITIC LOSS: 2.447909116744995\n",
      "CRITIC LOSS: 2.7194576263427734\n",
      "CRITIC LOSS: 1.6366374492645264\n",
      "CRITIC LOSS: 1.9050781726837158\n",
      "CRITIC LOSS: 2.275416612625122\n",
      "CRITIC LOSS: 3.4061622619628906\n",
      "CRITIC LOSS: 2.7376136779785156\n",
      "CRITIC LOSS: 2.0590078830718994\n",
      "CRITIC LOSS: 2.2642288208007812\n",
      "CRITIC LOSS: 2.7339563369750977\n",
      "CRITIC LOSS: 2.4256997108459473\n",
      "CRITIC LOSS: 2.3451528549194336\n",
      "CRITIC LOSS: 2.375030279159546\n",
      "CRITIC LOSS: 1.3019428253173828\n",
      "CRITIC LOSS: 2.4342713356018066\n",
      "CRITIC LOSS: 1.7654560804367065\n",
      "CRITIC LOSS: 1.9758363962173462\n",
      "CRITIC LOSS: 1.4270046949386597\n",
      "CRITIC LOSS: 2.005568027496338\n",
      "CRITIC LOSS: 3.6718103885650635\n",
      "CRITIC LOSS: 1.3695772886276245\n",
      "CRITIC LOSS: 3.2742249965667725\n",
      "CRITIC LOSS: 1.6212865114212036\n",
      "CRITIC LOSS: 2.350696086883545\n",
      "CRITIC LOSS: 1.2927998304367065\n",
      "CRITIC LOSS: 3.0285658836364746\n",
      "CRITIC LOSS: 1.780453085899353\n",
      "CRITIC LOSS: 2.427464246749878\n",
      "CRITIC LOSS: 3.080904245376587\n",
      "CRITIC LOSS: 2.2870423793792725\n",
      "CRITIC LOSS: 1.6874440908432007\n",
      "CRITIC LOSS: 1.9457426071166992\n",
      "CRITIC LOSS: 1.7417771816253662\n",
      "CRITIC LOSS: 1.8062543869018555\n",
      "CRITIC LOSS: 1.555371642112732\n",
      "CRITIC LOSS: 2.245985984802246\n",
      "CRITIC LOSS: 2.466735363006592\n",
      "CRITIC LOSS: 1.6297144889831543\n",
      "CRITIC LOSS: 1.9082317352294922\n",
      "CRITIC LOSS: 1.2615606784820557\n",
      "CRITIC LOSS: 2.0562362670898438\n",
      "CRITIC LOSS: 1.555037021636963\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[214]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m         memory.add_transition([observation, reward, terminated, action, next_observation])\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# train part\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(np.stack(rewards).mean().item())\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# env.close()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[214]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(step, delay)\u001b[39m\n\u001b[32m     19\u001b[39m q_optimizer.step()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m50\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCRITIC LOSS:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcritic_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % delay == \u001b[32m0\u001b[39m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(delay):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Reset the environment to generate the first observation\n",
    "\n",
    "import time\n",
    "\n",
    "def fit(step: int, delay: int = 3):\n",
    "    observation, reward, terminated, action, next_observation = memory.sample(256).T\n",
    "\n",
    "\n",
    "    action = torch.from_numpy(np.stack(action)).to(device)\n",
    "    next_observation = torch.from_numpy(np.stack(next_observation)).to(device)\n",
    "    observation = torch.from_numpy(np.stack(observation)).to(device)\n",
    "    terminated = torch.from_numpy(np.stack(terminated)).to(torch.float32).to(device)\n",
    "    reward = torch.from_numpy(np.stack(reward)).to(torch.float32).to(device)\n",
    "\n",
    "    critic_loss = compute_critic_loss(q_functions, policy, .2, .99, observation, action, reward, next_observation, terminated)\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"CRITIC LOSS At {step} :\", critic_loss.item())\n",
    "\n",
    "    if step % delay == 0:\n",
    "        for _ in range(delay):\n",
    "\n",
    "            actor_loss = compute_actor_loss(q_functions, policy, .2, observation, action)\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            # print(actor_loss.item())\n",
    "            ...\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "step = 0\n",
    "for epoch in range(5):\n",
    "    rewards = []\n",
    "    for _ in range(1_000):\n",
    "        step += 1\n",
    "        # this is where you would insert your policy\n",
    "        # start_time = time.time()\n",
    "        action, _ = policy.get_action(observations=torch.from_numpy(observation).to(device))\n",
    "        action = action.detach().cpu().numpy()\n",
    "        \n",
    "        # step (transition) through the environment with the action\n",
    "        # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if terminated:\n",
    "            observation = env.reset()\n",
    "\n",
    "        memory.add_transition([observation, reward, terminated, action, next_observation])\n",
    "\n",
    "        # train part\n",
    "        fit(step)\n",
    "\n",
    "    print(np.stack(rewards).mean().item())\n",
    " \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2e17e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action, _ = policy.get_action(observations=torch.from_numpy(observation).to(device))\n",
    "    action = action.detach().cpu().numpy()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c0b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee9b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianPolicy(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc_mean): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (fc_std): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eaae4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (fc1): Linear(in_features=3, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc_mean): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (fc_logstd): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83888f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.get_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbdf06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_action, log_prob = policy.get_action(next_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b33b2f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5134],\n",
       "        [1.1025],\n",
       "        [0.4588],\n",
       "        [0.1934]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([action, next_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36ec66f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0371],\n",
       "         [ 0.0396],\n",
       "         [ 0.2534],\n",
       "         [ 0.0683]],\n",
       "\n",
       "        [[-0.0111],\n",
       "         [ 0.0231],\n",
       "         [-0.1929],\n",
       "         [-0.0512]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = torch.stack([q_func.q_value(torch.concat([observation, next_observation]), torch.concat([action, next_action])) for q_func in q_functions])\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00c8ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, next_q = torch.split(qs, 2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5fdbc425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1929],\n",
       "        [-0.0512]], grad_fn=<MinBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q = torch.min(next_q, dim=0).values\n",
    "next_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "166699f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc04dca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7734],\n",
       "        [0.5635]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    next_q = next_q - alpha * log_prob\n",
    "\n",
    "next_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "818a8949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-90.1007, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q - (reward + 0.99 * next_q) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f96acff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0371],\n",
       "         [ 0.0396]],\n",
       "\n",
       "        [[-0.0111],\n",
       "         [ 0.0231]]], grad_fn=<SplitBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a25cd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f3dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f43249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ba781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.___torch_mangle_14.DummyModel,\n",
      "      %x : Float(2, 10, strides=[10, 1], requires_grad=0, device=cpu)):\n",
      "  %fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_12.Linear = prim::GetAttr[name=\"fc2\"](%self.1)\n",
      "  %act_func : __torch__.torch.nn.modules.activation.___torch_mangle_13.ReLU = prim::GetAttr[name=\"act_func\"](%self.1)\n",
      "  %fc1 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"fc1\"](%self.1)\n",
      "  %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%fc1)\n",
      "  %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%fc1)\n",
      "  %input.1 : Float(2, 50, strides=[50, 1], requires_grad=1, device=cpu) = aten::linear(%x, %weight.1, %bias.1), scope: __module.fc1 # /Users/nargizi/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0\n",
      "  %input : Float(2, 50, strides=[50, 1], requires_grad=1, device=cpu) = aten::relu(%input.1), scope: __module.act_func # /Users/nargizi/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/nn/functional.py:1697:0\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%fc2)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%fc2)\n",
      "  %42 : Float(2, 5, strides=[5, 1], requires_grad=1, device=cpu) = aten::linear(%input, %weight, %bias), scope: __module.fc2 # /Users/nargizi/opt/anaconda3/envs/rl/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0\n",
      "  return (%42)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb469fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f111b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
